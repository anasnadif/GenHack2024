{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWATySH3VIe0"
   },
   "source": [
    "# Extract content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-05T08:19:39.679093Z",
     "iopub.status.busy": "2024-03-05T08:19:39.678841Z",
     "iopub.status.idle": "2024-03-05T08:19:55.126071Z",
     "shell.execute_reply": "2024-03-05T08:19:55.124980Z",
     "shell.execute_reply.started": "2024-03-05T08:19:39.679070Z"
    },
    "id": "2Snk073U9-2O",
    "outputId": "e0bf3ea3-b286-4059-f01a-3bc72231aece"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.13.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (2.5.3)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n",
      "Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: httpcore, httpx, openai\n",
      "Successfully installed httpcore-1.0.4 httpx-0.27.0 openai-1.13.3\n"
     ]
    }
   ],
   "source": [
    "# if having an openai api\n",
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-05T08:41:00.682901Z",
     "iopub.status.busy": "2024-03-05T08:41:00.682304Z",
     "iopub.status.idle": "2024-03-05T08:41:01.542774Z",
     "shell.execute_reply": "2024-03-05T08:41:01.541902Z",
     "shell.execute_reply.started": "2024-03-05T08:41:00.682856Z"
    },
    "id": "6NGIbCCp9-2P"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "OPENAI_API_KEY=\"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\"\n",
    "client = OpenAI(api_key = OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9166,
     "status": "ok",
     "timestamp": 1708700384161,
     "user": {
      "displayName": "gestion rattrapages",
      "userId": "05033612024752985563"
     },
     "user_tz": -60
    },
    "id": "H05AExuKL4r9",
    "outputId": "5b382a20-afc2-43cf-e38e-e9164297c8e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting clean-text\n",
      "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
      "Collecting emoji<2.0.0,>=1.0.0 (from clean-text)\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting ftfy<7.0,>=6.0 (from clean-text)\n",
      "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.13)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=670182a348e924c0202676b3d4026bf8cea7ff04eaa6913b93110be88ed3c51c\n",
      "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji, ftfy, clean-text\n",
      "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.3\n"
     ]
    }
   ],
   "source": [
    "pip install clean-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1708700384880,
     "user": {
      "displayName": "gestion rattrapages",
      "userId": "05033612024752985563"
     },
     "user_tz": -60
    },
    "id": "fR1JHer5M1JO",
    "outputId": "dc6ad09b-71a6-4519-fe47-efba1c0aa70d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoupg\n",
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxgGLAtlMgbF"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    # patterns for irrelevant information\n",
    "    patterns = [\n",
    "      r'open\\s+in\\s+app',\n",
    "      r'sign\\s+(?:up|in|out|up for our newsletter)',\n",
    "      r'follow(?:ers)?',\n",
    "      r'listen',\n",
    "      r'share',\n",
    "      r'click\\s+here',\n",
    "      r'connect\\s+with\\s+me\\s+on',\n",
    "      r'twitter',\n",
    "      r'github',\n",
    "      r'instagram',\n",
    "      r'snapchat',\n",
    "      r'copyright',\n",
    "      r'facebook',\n",
    "      r'youtube',\n",
    "      r'tik\\s?tok',  # Added pattern for TikTok\n",
    "      r'help',\n",
    "      r'status',\n",
    "      r'about',\n",
    "      r'careers',\n",
    "      r'blog',\n",
    "      r'privacy',\n",
    "      r'terms',\n",
    "      r'\\b(?:login|register|account|contact|support|settings|notifications|download|upload|subscribe|email|password|username|logout)\\b',\n",
    "      r'\\d+\\s+min\\s+read',\n",
    "      r'written\\s+by\\s+\\w+\\s+\\w+',\n",
    "      r'\\d+\\s+followers',\n",
    "      r'--',\n",
    "      r'\\|'\n",
    "  ]\n",
    "\n",
    "    # Combine patterns into a single regex pattern\n",
    "    combined_pattern = '|'.join(patterns)\n",
    "    # Apply regex to remove irrelevant information\n",
    "    cleaned_text = re.sub(combined_pattern, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGSL5lnhLum3"
   },
   "source": [
    "##From_website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XKjkV78PbgT"
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://www.espn.com/\",\n",
    "    \"https://www.premierleague.com/\",\n",
    "    \"https://www.bikeradar.com/advice/beginners-cycling-tips\",\n",
    "    \"https://medium.com/tag/data-science\",\n",
    "    \"https://towardsdatascience.com/\",\n",
    "    \"https://medium.com/@avikumart_/a-complete-data-science-roadmap-to-follow-in-2023-3f8f837ddc06\",\n",
    "    \"https://www.tasteofhome.com/collection/travel-around-the-world-in-80-meals/\",\n",
    "    \"https://www.delish.com/cooking/menus/g1478/quick-dinner-ideas/\",\n",
    "    \"https://www.olivemagazine.com/recipes/collection/best-ever-french-recipes/\",\n",
    "    \"https://www.goodreads.com/quotes/tag/philosophy\",\n",
    "    \"https://plato.stanford.edu/entries/aristotle-metaphysics/\",\n",
    "    \"https://medium.com/stoicism-philosophy-as-a-way-of-life/stoicism-and-its-importance-in-the-21st-century-4b5c6a477722\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4119,
     "status": "ok",
     "timestamp": 1708700828863,
     "user": {
      "displayName": "gestion rattrapages",
      "userId": "05033612024752985563"
     },
     "user_tz": -60
    },
    "id": "bom-VPFOfJWe",
    "outputId": "7f9a9ef9-cbfa-4324-aef1-fe772893111b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error accessing URL https://medium.com/stoicism-philosophy-as-a-way-of-life/stoicism-and-its-importance-in-the-21st-century-4b5c6a477722: HTTP Error 410: Gone\n"
     ]
    }
   ],
   "source": [
    "for i, url in enumerate(urls):\n",
    "    try:\n",
    "        req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        web_byte = urlopen(req).read()\n",
    "        webpage = web_byte.decode('utf-8')\n",
    "        soup = BeautifulSoup(webpage, 'html.parser')\n",
    "        all_text = soup.get_text(separator='\\n', strip=True)\n",
    "        cleaned = clean(all_text, no_emoji=True, no_line_breaks=True)\n",
    "        cleaned = clean_text(cleaned)\n",
    "        with open(f'cleaned_text_{i}.txt', 'w') as file:\n",
    "            file.write(cleaned)\n",
    "    except HTTPError as e:\n",
    "        print(f\"Error accessing URL {url}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aw-jpgZgmMn"
   },
   "source": [
    "## youtube links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 820,
     "status": "ok",
     "timestamp": 1708702081950,
     "user": {
      "displayName": "gestion rattrapages",
      "userId": "05033612024752985563"
     },
     "user_tz": -60
    },
    "id": "Z-IpZdW-lHzW",
    "outputId": "855d01c0-1a76-4f8e-cdcb-33e4f1406839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: youtube-transcript-api\n",
      "Successfully installed youtube-transcript-api-0.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install youtube-transcript-api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4Dw2XJEkfeP"
   },
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLFPZTemipYS"
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_youtube_text_content(video_link):\n",
    "    try:\n",
    "        # Extract video ID from the link\n",
    "        video_id = video_link.split(\"v=\")[1]\n",
    "        if '&' in video_id:\n",
    "            video_id = video_id.split('&')[0]\n",
    "\n",
    "        # Get transcript\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "\n",
    "        # Extract text content from transcript\n",
    "        text_content = \"\"\n",
    "        for segment in transcript:\n",
    "            text_content += segment['text'] + \" \"\n",
    "\n",
    "        return text_content.strip()  # Remove trailing whitespace\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1252,
     "status": "ok",
     "timestamp": 1708702125598,
     "user": {
      "displayName": "gestion rattrapages",
      "userId": "05033612024752985563"
     },
     "user_tz": -60
    },
    "id": "cF_jys4ClNav",
    "outputId": "9eb05d74-082d-45f1-d911-b5463bb8231d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text content extracted successfully:\n",
      "hello everyone welcome to gianti a channel where you get to know everything about data science okay so so far we have covered seven different sessions on named entity recognition and relation extraction we started by understanding the Nerf and relation extraction then we went about doing the level leveling through the level Studio then we had the custom many art done through Spacey spaceky Transformers anti-transformers and in the previous session the last two sessions in fact we learned a lot about relation extraction using reform right in this session we intend to be discussing uh another model state of the art model for relation extraction that is Ripple okay so Ripple essentially stands for relation extraction Pi end-to-end language generation it uses Auto regressive sequence to sequence model so what are Auto regressive sequence to sequence models think of them as the language generation models like the T5 or depart and so on like where you would be having the encoded component and also the decoder component if we see how Ripple performs on the different data sets you'll find it really does a very good job on multiple different types of the data sets like on the atmos truck events it's a scoring a rank of three on panel zero four again a rank of three or one and talk rate is rank of 2 1 and there are several other data set where it does really very well so that's why uh I thought that it will be useful to discuss about repel as one of the Frameworks for extracting the relations from the text okay so what's the key mechanism how it operates so like I stated uh it uses Auto regressive sequence to sequence models these are the type of the sequence to sequence model which have a series optic components an encoder which processes a sequence of the input data and the decoder which produces a sequence of the output data in case of repel it uses the part large as the uh as the base model right it expresses triplets as sequence of tokens such that we can retrieve the original relations and minimize the number of the tokens to be generated thus to make the coding more efficient okay this step essentially is called the triplet linearization process on our step and will know a little bit more about it in fact for this purposes it uses a set of the new tokens or as markers to achieve the triplet linearization so you'll find you'll find that uh less than a triplet greater than mark this particular mark it uses for denoting the start of a new triplet with the new head entity followed by the surface form of the entity in the input text surface form meaning the entire text which is under the subject entity the entire text part of the surface entire text part of the subject entity these are less than uh sub s u b z greater than marks the end of the head entity and the start of the tail entity surface more right so if they are um a single subject with multiple different objects then and there will be a single sup Z related to entity odd marker and then it will be followed with all the other op say related entity we'll see the implementation of this in the next slide the obj marks the end of the tail entity and the start of the relation between the head and tail entity in its surface form right okay let's see one of the examples uh from the paper itself so it says this must be the place is a song My New Wave band Talking Heads so uh so the name of the song is this must be the place and uh and the chainer of the band is New Wave and the band team is Talking Heads okay and it was released in November 1983 as the second single from its fifth album speaking in tongues so if we see the various triplets that get extracted from this text it's as below this must be the place uh that's the subject and the performer is the relation and Talking Heads is the object right so this must be the place and the performer the name of the band is essentially the Talking Heads now Talking Heads is not each inner engineer New Wave right so that's what is here the second uh triplet then it also says this must be the place that's the subject and is part of speaking in tongues so the name of the album in fact is speaking in the tongues and it's the part of this album right this song is the part of the self so that's why we have this as the subject relation is part of and object is speaking in tongues now speaking in the tongues in tongues is again an album which is uh and the performer for this album is Talking Heads okay the band name is Talking Heads Okay so we have to essentially extract all these relations and the way repel actually generates uh the triplet linearization is like here so we have the start of the triplet here this must be this place so like we saw in the previous slide this is the subject Talking Heads is the object and performer is the relations uh expressed between the subject and the object now the second again uh the PC the second subject here s-u-p-j now this is essentially the marker 40 this must be the place the first one uh itself right because now we are trying to express this third relationship which you see in the uh in the left hand side bottom box right so where we have this must be the place has the subject part of physical relation and through speaking in tongues is the object so here our subject continues to be this must be the place and object is speaking in tongues and part of is the relation expressed between the subject and object the second triplet is Talking Heads now that is the subject we have and New Wave is a sensility object and genome is the sensility relations between the subject and object the third triplet that we have is the speaking in tongues and which is this subject Talking Heads is the object and performer is the relation expects expressed between subject and the object so this way is how the triplet linearization is achieved in case of a variable and this makes sure that the number of the tokens generated in the process is most optimal okay so you might want to pause a bit here and give it another thought as to how this nurture plate was essentially obtained just keep in mind that there are three different triplets or three different or unique subjects here this must be the place Talking Heads and speaking in tongues okay these are only three different or unique subjects and they are to be marked with the different objects and relationships that's what is getting done in the right hand sign box okay so then uh moving on the mechanics let's talk about the mechanics and demo so we'll be essentially referring to these two GitHub repo Babel Escape Rebel and Joint here a Rebels for these code purposes and their joint tier for some of the data that you might be wanting to experiment like the nyt data set and so on okay so now let's get into the mechanics part of it okay so here we are on the papal Escape repel this GitHub repo so this is how you will find the different uh directories con data data sets talks experiments SRC and so on they also have got the code for demo dot pi and we also have the requirements.txt setup.sh and this PC component dot pi okay so if we see the repo structure uh it contains it has the con uh directory which contains the Hydra config files for data model and train and then uh it also talks about the model as a repository where model files are to be stored and SRC contains the various other python files containing say for example PL underscore data underscore modules dot Pi it has the lightning data module PL underscore modules dot Pi has the lighting module and so on hand train dot Pi is the main script for the training for training Network and so on okay so for uh what I have done is I have downloaded thetrepo and got my ID set so this is how the things look like on my ID so I have the model directory and I'm calling it as Babel is a the reason is that several places our authors have in fact used uh this particular part Bible Escape slash or stroke rather and we have the Ripple large okay so that's what authors have used at several cases so I'm calling it as mammal escape and uh then uh so in this essentially has the model so you have got the configuration vocabulation by torch underscore model dot event then con like we discussed it has got the Hydra files and essentially it has the con for config for data so depending upon whatever kind of the data that you use say adverse event or kernel or if it is the default data then default underscore data Yahoo so this is how it looks where I've made certain changes here so and the default version you'll find the number equals to be 6 right I have made it to B1 because of my current hardware issues right and the data set name again I have changed the paths here so depending upon how your directories and how your local environment is set you may want to please change the path accordingly similarly uh if instead you want you have your data available in the nyt underscore data format then you may want to change the paths and so on here write the default one that you'll find is home multi-range triple data sets and so on so we need to change this okay so that's on the data side config model side of the config default underscore model nothing much to be done here they use like the paper state that they use bot large so that's what is here has the base model on the train side uh default underscore train again I have made a lot of changes here just to make it work so I'm using uh the pad size of one instead of four for eval I'm using again a batch size of one instead of 8 which is the default one right so depending upon your Hardware kindly make the changes here right so that's on the train side uh right and on the data now this particular folder essentially uh contains another subfolder called repel I have added nyt subfolder here and I have kept the three different data which are Dev test and train this data is what you can get from uh join here so you can go to their data sets and from here you can get the data right and YT uh hyphen multi Theta okay so uh so in just in case you want to see the performance of it on the nyt data set um the uh it also comes with a sample contort Json L and which is for the default version of their model if we see this data this is how it's look it looks like it's been trained on this kind of a data set and the use an annotation tool called crocodile um so if you see uh it has it has got these different elements so we have talk ID and then the title URI and then we have the text and for the text we have got uh the entities now the entities they again have got the uh URI unique resource identification I believe that's the full form of URI and then it has gone to boundaries and its surface form uh what we were discussing earlier in this slide has got a text here a cutting Journal and who was the annotator so I think that's the me for me so that's how it's this data is represented they have provided this sample file which contains some thousands of the records here and essentially you'll find that their demo or the streamlit demo that they have provided runs on this sample.j Chanel if we need to uh train it on the original data then again you please download it from uh from this repo so you can download the model and the data site files from here so sf4.io it would look like this so we need to download Rebel both the data sets these are some 1.4 GB files so we need to download both of them okay so that was about the data then the data sets it contains the Python scripts essentially for the pre-processing purposes so when we go to nyt underscore type dot Pi you'll find uh the mappings so originally nyt represents the uh the subject object and relationships in this move uh basic corresponding mapping which has been provided in the document and uh and theta's topic quotes are there to get it in the requisite format right or do the unnecessary pre-processing then we have the placeholders for experiment we have the outputs which essentially contains the uh Hydra file so you'll find dot Hydra and a config.eml and then hydra.eml overrides so these are the final configs that get applied so these are the files on which the model has been trained and like I stated I downloaded it from OS right and uh so that's about it and then the SRC this contains several of these python files uh gen for generating the samples model savings modeling part and so on now since they have been put under srcs and my root directory is repel hyphen mean hence at several places I have to actually change the code when say importing a particular module from these Python scripts so I essentially would have to do SRC torch in rate underscore samples right to import a particular function from here so which I have met the necessary changes at several places in my code like here from SRC dot generate samples import generate examples call back okay so I have added SRC dot just to make sure that it works as per my current setup okay so then uh we have got uh everything set with this for the repo actually provides some bit of information on how to go about it so they have provided uh the code for for the very basic experimentation purposes so we'll try that so for that I have created a file called main.i and essentially copy pasted this this entire portion to here okay so we'll try this it says from Transformer simple pipeline so we do that then uh triplet underscore extractor pipeline text to text generation model is bevel Escape Ripple large and again tokenizer is triple large so I have just initiated that and it takes a couple of seconds so we have this pipeline ready all right and uh the extracted text here my text is this Punta Cana is a resort town in the municipality of of higai sorry for the map pronunciation okay so this one okay so we get the extracted text here uh so I just ran it and uh we can print it as well so we get the extracted text or a triplet here Punta Cana is the subject and law alter Glacier province is the object located in the is the relation now to get it in the page metaphor it says it's a function to pass the generated text and the extracted triplets so we will run this and okay so this one we get it as head is Punta cannot type is located in the tail is La pentagration province okay now this one is clear now they also have provided uh the means to add it to the Spacey pipeline right so that's what is here so we can take that uh portion of the code so we have him we we need to do import SPC it's not there in requirements.txt so you will have to separately install this PC and please make sure that you do spc3.x and we just ran this PC underscore component NLP will load the Encore webassem model and I've already added the pipe here so I do not need to add it the second time but if you are trying it for the first time please make sure that you have added this pipe okay so uh essentially this one and my input sentence is there and I will first create the species document and then I will create a doc list and then get the relationships and the corresponding value out from here Okay so so essentially uh uh so so here uh we'll run this so a top thought well we'll be getting something like this uh uh relations located Administration so this is essentially the text which we should be getting when we have learned the Spacey pipeline it's not coming for me but then uh this is what we should be getting okay so which gives you this pan here and then it tells you the relations at the headspan and the tail span is given there okay so that was about the main dot pi and how do we sort of addict to uh this pacing next thing is how do we train and replicate whatever we have it on the uh repel underscore large for that we need to take the default data set and uh we just need to run the screen and train dot pi so it starts with this and uh uh it will be importing all the libraries and so on Mini and uh it will be seating a global value in this case it is 42 which says special tokens have been added in the vocabulary and then it will go about performing the rest of the task as well okay so some of the sorry uh here like it will start easing reading the GPU whether it's available or not and then it will also uh do some validation and then it will create the model okay uh so maybe you can give it a try once you have done the complete setup on your local and one last thing which I quickly wanted to talk about is uh is essentially uh the the demo uh the demo dot Pi okay so we'll see this uh the demo dot Pi uh we have here the streamlib demo or related code which they have made available so this is the version of the code what I have done is I'm running it separately online through my command prompt okay and uh now this is also available so I'll go to or demo streamlit so this is where we have the code and uh it what it does here is maybe I'll make it a little larger so what it does here is it asks you for the data set ID I can set it anything between zero two thousand it essentially will pick the text from the sample dot additional ingestion L that we saw earlier right so similarly and then it asks for some of the other parents like left penalty num teams and written sequences Etc in the in the the current input text is cello Peck Hill is the ice covered Hill rising to 946 meter in the north foothills of Detroit Plateau on Trinity Peninsula Ain Graham Land Antarctica and these are the outputs that's been provided so if we have something called the silver output and here the triplet is essentially Trinity Peninsula is the subject Graham Land is the object part of is the relationship and again uh Trinity Peninsula is the subject right and that's why we have each subject here and uh Antarctica is the object and uh and Antarctica is essentially the continent so Antarctica continent has Trinity peninsula in it okay so that's what this relationship suggests similarly we have the second triplet here now which is Graham Land subject Antarctica is the object and continent is the the relationship time so that's what is presented here in a cleaner way and and then it also gives you the triplets for each of these sentences sentence zeros sentence one and likewise let's try for uh something else as well so maybe I will pick something say randomly I'll pick the eight of the eighth text from the sample so it says the input text is essentially its surfaced in the first half of the 14th century at Sand rumbles cathedral in city of Michelin pardon me for my bad pronunciation anywhere right and it says this lower output looks like this and trumbull's Cathedral is the subject Michelin is the object located in the administrative tutorial entity okay um so it's the located in the administrative factorial is the entity okay so that's what is the relation which is in uh now predicted here okay sounds great if I want to give my own input then uh we can give it from here right so say for example if I say um India gate is in New Delhi India say for example if I want to give it this text press into India Gate is in New Delhi India okay so let's see what it does so it says the prediction text is India Gate is the subject and New Delhi is the object located in the administrative perfect okay and it is the subject India Gate is the subject India is the object and Country which is again fine triplet uh the another one is uh New Delhi is the subject India is the object and country is the relationship perfect I think it got it right here so that's how it works we can play with it and we can train in Ripple model specifically for our purposes and to get the different relationship extracts as needed for our requirements so for our purposes right hope you like this video stay tuned there are a lot more things to come up and going forward will Embark onto a new journey we'll pick up one another thread one another important topic in the text Mining and image mining or image processing and will delve deep into it so idea is to get into the mechanics get the understanding of the things and then it should trigger some thought process in you and if you might become a little more comfortable in doing the day-to-day stuff right at your workplace or even otherwise so thank you thanks for your time\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "video_link = \"https://www.youtube.com/watch?v=Y1RoBPmSOJQ&t=1012s&ab_channel=GyanDEEP\"\n",
    "text_content = extract_youtube_text_content(video_link)\n",
    "\n",
    "if text_content:\n",
    "    print(\"Text content extracted successfully:\")\n",
    "    print(text_content)\n",
    "else:\n",
    "    print(\"Failed to extract text content.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BEa2S0AlWsT"
   },
   "outputs": [],
   "source": [
    "f= open(\"text_content_youtube.txt\")\n",
    "f.write(text_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lOguweC9-3Z"
   },
   "source": [
    "In the following sections, we only use OpenAI api\n",
    "\n",
    "## Treating the Image Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-05T08:41:14.115882Z",
     "iopub.status.busy": "2024-03-05T08:41:14.115411Z",
     "iopub.status.idle": "2024-03-05T08:41:14.597964Z",
     "shell.execute_reply": "2024-03-05T08:41:14.597196Z",
     "shell.execute_reply.started": "2024-03-05T08:41:14.115854Z"
    },
    "id": "Q42UkGkv9-3a"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"/kaggle/input/vit-image/VIT.png\"\n",
    "\n",
    "# Getting the base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "headers = {\n",
    "  \"Content-Type\": \"application/json\",\n",
    "  \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "  \"model\": \"gpt-4-vision-preview\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"give a brief summary of what is presented in the image\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 300\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "result = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T00:18:20.363018Z",
     "iopub.status.busy": "2024-02-29T00:18:20.362114Z",
     "iopub.status.idle": "2024-02-29T00:18:20.368607Z",
     "shell.execute_reply": "2024-02-29T00:18:20.367565Z",
     "shell.execute_reply.started": "2024-02-29T00:18:20.362980Z"
    },
    "id": "bCw5goy89-3b",
    "outputId": "ae01f859-fdd1-4565-9ac6-60d4bc62243e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image presents a schematic illustration of the Vision Transformer (ViT) architecture, which is a type of artificial intelligence model used for image processing tasks. The process starts with an input image that gets divided into a series of smaller image patches. These patches are then flattened and linearly projected into a sequence of vectors (embedded patches). Additionally, position embeddings are added to these vectors to maintain positional information.\n",
      "\n",
      "The sequence of vectors, along with an extra learnable class embedding, is passed through multiple layers of the transformer encoder. Each transformer encoder layer is composed of multi-head self-attention mechanisms and multilayer perceptrons (MLPs), with normalization (Norm) layers in between. The attention mechanism allows the model to focus on different parts of the image when making decisions.\n",
      "\n",
      "After processing the sequence through the transformer encoder, the class embedding (which is initially arbitrary and learns to represent the whole image during training) is used by a MLP head to predict the class of the image (e.g., bird, ball, car, etc.).\n",
      "\n",
      "On the right side of the diagram, there's a more detailed illustration of the inner workings of a transformer encoder layer, showing the flow of data through the multi-head attention and MLP, as well as the residual connections around them.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-Jh0KMX9-3c"
   },
   "source": [
    "## Treating the Audio Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:31:25.797148Z",
     "iopub.status.busy": "2024-02-28T14:31:25.796775Z",
     "iopub.status.idle": "2024-02-28T14:31:38.937281Z",
     "shell.execute_reply": "2024-02-28T14:31:38.936563Z",
     "shell.execute_reply.started": "2024-02-28T14:31:25.797118Z"
    },
    "id": "zaSMXEGQ9-3c"
   },
   "outputs": [],
   "source": [
    "audio_file= open(\"/kaggle/input/arthur/Arthur.mp3\", \"rb\")\n",
    "transcription = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\",\n",
    "  file=audio_file\n",
    ")\n",
    "result = transcription.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:32:20.498438Z",
     "iopub.status.busy": "2024-02-28T14:32:20.498061Z",
     "iopub.status.idle": "2024-02-28T14:32:20.505661Z",
     "shell.execute_reply": "2024-02-28T14:32:20.504784Z",
     "shell.execute_reply.started": "2024-02-28T14:32:20.498404Z"
    },
    "id": "6fHBaVKl9-3d",
    "outputId": "09a16039-4ea3-4be6-a75b-cd59fd03fe3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The story of Arthur the Rat. Once upon a time, there was a young rat who couldn't make up his mind. Whenever the other rats asked him if he would like to come out hunting with them, he would answer in a hoarse voice, I don't know. And when they said, would you rather stay inside, he would say yes or no either. He'd always shake making a choice. One fine day, his Aunt Josephine said to him, now look here, no one will ever care for you if you carry on like this. You have no more mind of your own than a greasy old blade of grass. The young rat coughed and looked quiet as usual, but said nothing. Don't you think so, said his aunt, stamping her foot, for she couldn't bear to see the young rat so cold-blooded. I don't know, was all he ever answered. And then he walked off to think for an hour or more, whether he would stay in his hole in the ground or go out into the loft. One night the rats heard a loud noise in the loft. It was a very dreary old place. The roof let the rain come down, come washing in. The beams and the rafters had all rotted through. So the whole thing was quite unsafe. At last one of the joists gave way, and the beams fell with one edge on the floor. The walls shook, the cupola fell off, and all the rats' hair stood on end with fear and horror. This won't do, city leader. We can't stay cooped up here any longer. So they sent out scouts to search for a new home. A little later on that evening, the scouts came back and said they'd find an old-fashioned art farm where there would be room for all of them. The leader gave the order at once. Come new fallen in. And the rats crawled out of the hole right away and stood on the floor in a long line. Just then the old rat caught sight of young Arthur. That was the name of the shaker. He wasn't in the line, and he wasn't exactly outside it. He stood just by it. Come on, get in line. Growled the old rat coarsely. Of course you're coming too. I don't know, said Arthur calmly. Why, the idea of it. You don't think it's safe here anymore, do you? I'm not certain, said Arthur undaunted. The roof may not fall down yet. Well, said the old rat, you can't wait for you to join us. Then he turned to the other and shouted right about the face. Marked. And the long line marched out to the barn while the young rat watched him. I think I'll go tomorrow, he said to himself. But then again, perhaps I won't. It's so nice and snug here. I guess I'll go back to my hole under the log for a while just to make up my mind. But during the night there was a big crash. Down came beams, rafters, joists, the whole business. Next morning, it was a foggy day, some men came to look over the damage. It seemed odd to them that the old building was not haunted by rats. But at last one of them happened to move aboard, and he caught sight of a young rat, quite dead, half in and half out of his hole. Thus the shirker got his due and there was no mourning for him.\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFxWwEHDiiwm"
   },
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5MFDGWSlsBX"
   },
   "source": [
    "##Tf-idf inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HP48NxYfnX3W"
   },
   "outputs": [],
   "source": [
    "#**************************************\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "### Remove stopwords and punctuations.\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2975,
     "status": "ok",
     "timestamp": 1708262181150,
     "user": {
      "displayName": "gestion rattrapages",
      "userId": "05033612024752985563"
     },
     "user_tz": -60
    },
    "id": "ECQqCJPKbFcX",
    "outputId": "48a2e50d-d207-4128-c8b7-ff8e7ef19b19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1708262164138,
     "user": {
      "displayName": "gestion rattrapages",
      "userId": "05033612024752985563"
     },
     "user_tz": -60
    },
    "id": "kWtBTd7WaOed",
    "outputId": "4b836c29-1450-49e2-df22-41bc68bf91a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  espn - serving sports fans. anytime. anywhere....\n",
      "1  premier league live scores, stats & 2023/24 sk...\n",
      "2  cycling for beginners 25 essential tips for ne...\n",
      "3  the most insightful stories data science - med...\n",
      "4  towards data science homepage get started towa...\n",
      "5  a complete data science roadmap to in 2023 by ...\n",
      "6  95 international recipes to make when you're c...\n",
      "7  80 best quick dinner ideas - easy fast dinner ...\n",
      "8  37 french recipes olivemagazine podcast video ...\n",
      "9  philosophy quotes (27706 quotes) , home my boo...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_contents = []\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    with open(f'cleaned_text_{i}.txt', 'r') as file:\n",
    "        text_content = file.read()\n",
    "\n",
    "        text_contents.append(text_content)\n",
    "\n",
    "df = pd.DataFrame({'text': text_contents})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtcHpNw6bA0j"
   },
   "outputs": [],
   "source": [
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2IAxncF6aGmV"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Function to preprocess text\n",
    "def text_preprocess(text):\n",
    "    # Remove punctuation\n",
    "    trans = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(trans)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return cleaned_text\n",
    "\n",
    "data['text'] = data['text'].apply(text_preprocess)\n",
    "\n",
    "# Function to generate inverted index\n",
    "def generate_inverted_index(data):\n",
    "    inverted_index = {}\n",
    "    for index, doc_text in enumerate(data):\n",
    "        for word in doc_text.split():\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = [index]\n",
    "            elif index not in inverted_index[word]:\n",
    "                inverted_index[word].append(index)\n",
    "    return inverted_index\n",
    "\n",
    "# Apply inverted index generation to the DataFrame\n",
    "inverted_index = generate_inverted_index(data['text'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTvKB7csbV0M"
   },
   "outputs": [],
   "source": [
    "inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1708196351784,
     "user": {
      "displayName": "gestion rattrapages",
      "userId": "05033612024752985563"
     },
     "user_tz": -60
    },
    "id": "yetWPRpsb79f",
    "outputId": "01f26543-cb47-426c-ca8a-7825f656f923"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4454)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docvectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1708262692625,
     "user": {
      "displayName": "gestion rattrapages",
      "userId": "05033612024752985563"
     },
     "user_tz": -60
    },
    "id": "5EOXVdxMfmig",
    "outputId": "1d7e7424-5936-4c16-848d-fc24876c8aec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8, 2, 6, 7}\n"
     ]
    }
   ],
   "source": [
    "def search_documents(query, inverted_index, data):\n",
    "    cleaned_query = text_preprocess(query)\n",
    "    query_terms = cleaned_query.split()\n",
    "\n",
    "    relevant_documents = set()\n",
    "\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index:\n",
    "            term_documents = set(inverted_index[term])\n",
    "            if not relevant_documents:\n",
    "                relevant_documents = term_documents\n",
    "            else:\n",
    "                relevant_documents = relevant_documents.intersection(term_documents)\n",
    "\n",
    "    # Retrieve the documents corresponding to the relevant document indices\n",
    "    relevant_docs = [data[idx] for idx in relevant_documents]\n",
    "\n",
    "\n",
    "    return relevant_documents ,relevant_docs\n",
    "\n",
    "\n",
    "query = \"delicious meal\"\n",
    "relevant_documents ,relevant_docs = search_documents(query, inverted_index, data['text'].tolist())\n",
    "print(relevant_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHwgOXPAdb55"
   },
   "outputs": [],
   "source": [
    "### Generate document embeddings with simple TF-IDF\n",
    "def retrieve_vectors(text):\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    docvectors = tfidf_vectorizer.fit_transform(text).toarray()\n",
    "    return tfidf_vectorizer, docvectors\n",
    "\n",
    "tfidf_vectorizer, docvectors = retrieve_vectors(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1708276678586,
     "user": {
      "displayName": "gestion rattrapages",
      "userId": "05033612024752985563"
     },
     "user_tz": -60
    },
    "id": "Ub4x1lAwddEJ",
    "outputId": "3f9b02d2-8173-44bf-c4d4-2e26a4ffdcf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 similar documents:\n",
      "Document 1: insightful stories data science medium write explore topics data science technology programming self improvement writing relationships deep learning data science topic 45m ers 297k stories recommended stories radmila towards data science creating better dashboardmyth reality beta version 20 built using dash plotly instead matplotlib 1 day ago tea mustac towards data science emotionsintheloop analyzing life scanned jane 1 day ago 1 dave melillo towards data science building data platform 2024 build modern scalable data platform power analytics data science projects updated feb 5 2024 10 10 vincent koc towards data science explaining openai soras spacetime patches key ingredient hood generative ai video openai 1 day ago 2 2 simon hawe prosiebensat1 tech nvidia triton unleashed elevate apps performance game compute recommendations realtime low latency 2 days ago 1 1 radmila towards data science creating better dashboardmyth reality beta version 20 built using dash plotly instead matplotlib 1 day ago tea mustac towards data science emotionsintheloop analyzing life scanned jane 1 day ago 1 dave melillo towards data science building data platform 2024 build modern scalable data platform power analytics data science projects updated feb 5 2024 10 vincent koc towards data science explaining openai soras spacetime patches key ingredient hood generative ai video openai 1 day ago 2 simon hawe prosiebensat1 tech nvidia triton unleashed elevate apps performance game compute recommendations realtime low latency 2 days ago 1 mariya mansurova towards data science text embeddings comprehensive guide evolution visualisation applications text embeddings 5 days ago 12 egor howell towards data science would learn 2024 zero would go learning one worlds popular programming languages 1 day ago 1 samuel flender towards data science rise sparse mixtures experts switch transformers deepdive technology paved way capable llms industry today 2 days ago see recommended stories\n",
      "Document 2: complete data science roadmap 2023 avikumar talaviya medium write complete data science roadmap 2023 avikumar talaviya feb 7 2023 6 looking start learning data science 2023 heres roadmap photo lukas blazek unsplash introduction data science rapidly growing field becoming demand organizations around world aspiring professionals fields looking ways jumpstart data science journey order stay competitive industry achieve goal present endtoend data science roadmap data science aspirants 2023 roadmap cover essential topics skills needed become successful provide starting point wanting break data science industry furthermore discuss various resources tools available data science aspirants use provide overview current data science landscape ing sections provide overview data science roadmap discuss topics skills required outline available resources tools note article originally published datakwery worlds site search data science resources one place link original article data science roadmap outline programming language fundamentals statistics mathematics essentials data wrangling visualization using pandas numpy matplotlib seaborn scipy libraries jupyter notebook enviroment sql programming languages mysql data warehouse using scikit learn deep learning using keras nlp concepts techniques deployment using streamlit flask docker awsazuregcp portfolio projects interview prep job application programming language fundamentals one widely used programming languages todays time named one popular programming languages according stackoverflow developers survey 2022 list fundamentals one learn basic data types int float str bool variables assignment operators control flow statements ifelse statements loops functions modules lists tuple arrays dictionaries sets exception handling objectoriented programming oop concepts classes objects methods inheritance file inputoutput operations basic regular expressions 2 statistics mathematics essentials learning programming language learn statistics mathematics essentials learn data science tech stack become proficient descriptive statistics measures central tendency mean median mode measures variability range standard deviation variance measures shape skewness kurtosis probability basic probability concepts conditional probability bayes theorem random variables along probability distributions estimation hypothesis testing bayesian methods must inferential statistics concepts estimation hypothesis testing pvalues confidence intervals linear algebra concepts vectors matrices matrix operations important understanding linear regression algorithms calculus concepts gradient partial derivatives optimization important understanding ml algorithms multivariable calculus concepts gradients jacobian hessian optimization important understanding neural networks algorithms time series analysis concepts moving averages exponential smoothing arima models 3 data wrangling jupyter notebook environment data wrangling data manipulation crucial skill develop wide range libraries perform variety data manipulation tasks visualize data distributions find key insights datasets libraries pandas numpy seaborn matplotlib plotly sklearn scipy one master become expert data wrangling visualizations tasks libraries step data wrangling includes tasks cleaning transforming merging data various sources proficient using libraries pandas numpy tasks data exploration includes tasks identifying patterns outliers anomalies data proficient using libraries matplotlib seaborn exploration data transformation includes tasks normalization encoding categorical variables scaling data proficient using libraries sklearn tasks feature engineering includes creating new features existing data selecting relevant features handling missing data proficient creating various types visualizations line charts bar charts scatter plots heat maps using libraries matplotlib seaborn data storytelling able present data insights findings nontechnical stakeholders clear compelling way 4 sql programming language mysql database along also proficient sql programming language store manipulate relational databases order work vast amount data key sql concepts master select statement used query retrieve data database table join clause used combine rows multiple tables based related column group clause used group rows based one columns perform aggregate functions sum count avg clause used filter rows based certain condition subquery inner join used combine data multiple tables filter results indexing used improve performance queries creating index one columns table create alter statements used create modify structure tables database objects insert update delete statements used insert update delete data table advanced concepts like window functions common table expressions ctes stored procedures 5 using scikit learn doubt integral part data science processes across industries good grasp programming language libraries learn handson using scikit learn library key concepts skills master using scikitlearn supervised learning concepts regression classification algorithms linear regression logistic regression decision trees unsupervised learning concepts clustering dimensionality reduction algorithms kmeans hierarchical clustering pca model evaluation techniques training testing sets crossvalidation metrics accuracy precision recall f1score hyperparameter tuning techniques grid search random search optimize performance model feature selection engineering techniques select relevant features create new features existing data pipelines techniques chain multiple steps process data preparation feature selection model training single scikitlearn estimator ensemble methods concepts bagging boosting algorithms random forest gradient boosting neural networks understanding concepts usage mlp neural network architectures 6 deep learning using keras deep learning powerful technique one learn may tackle unstructured data like images text video etc wherein deep learning techniques play crucial role key deeplearning techniques learn using keras artificial neural networks concepts feedforward networks backpropagation activation functions convolutional neural networks cnns used image classification object recognition tasks recurrent neural networks rnns used sequential data text time series autoencoders used unsupervised feature learning dimensionality reduction generative models generative adversarial networks gans variational autoencoders vaes transfer learning techniques using pretrained models vgg resnet improve performance new task hyperparameter tuning techniques grid search random search optimize performance deep learning model tensorboard visualize training performance deep learning model 7 natural language processing nlp techniques concepts nlp subfield leverages analysis generation understanding human languages order derive meaningful insights key nlp concepts techniques learn text preprocessing techniques tokenization stemming lemmatization convert raw text format easily analyzed text feature extraction techniques bagofwords ngrams word embeddings represent text numerical features use models text sort techniques classifying text predefined categories sentiment analysis spam detection named entity recognition techniques identifying extracting named entities text people organizations locations partofspeech tagging techniques identifying parts speech words sentence nouns verbs adjectives text generation techniques generating new text based given input machine translation text summarization texttospeech speechtotext techniques converting speech text advanced concepts like attentionbased models transformers bert 8 model deployment data science jobs require high level skill developing quality models good understanding experience deploying models would give edge model serving techniques serving models production environment using rest api dedicated model server containerization techniques packaging models dependencies container docker ensure consistent reproducible deployments cloud deployment techniques deploying models cloud platforms aws sagemaker azure google cloud ml engine 9 portfolio projects learning skills time build portfolio projects showcase potential recruiters skills expertise subjects project ideas work predicting road accident severity energy intensity prediction wildblue berry prediction patient survival prediction cyberbullying detection using nlp techniques nextword prediction project 10 interview prep job application time prepare interviews apply jobs suitable tips data science job interviews understand company job research company specific role applying understand goals values type work brush key skills review practice key skills required job programming languages statistical analysis techniques prepare common interview questions prepared answer common data science interview questions experience x technique would solve problem practice databased questions ready answer databased questions data analysis problems would analyze dataset approach building model able explain work able explain past projects methods used clear concise manner show passion show enthusiasm data science willingness learn grow field prepared ask questions prepare list thoughtful questions ask interviewer company role team working queries regarding article data science 6 ger httpscomavikumart\n",
      "Document 3: towards data science homepage get started towards data science home data science medium publication sharing concepts ideas codes latest editors picks deep dives author resources newsletter ing database data transformation data engineers database data transformation data engineers advanced techniques beginners mike shakhomirov feb 17 introduction finetuning pretrained transformers models introduction finetuning pretrained transformers models simplified utilizing huggingface trainerobject ram vegiraju feb 17 latest quantized mistral 7b vs tinyllama resourceconstrained systems quantized mistral 7b vs tinyllama resourceconstrained systems performance comparison models accuracy response time rag questionanswering setup kennedy selvadurai phd feb 17 would learn 2024 zero would learn 2024 fromzero would go learning one worlds popular programming languages egor howell feb 16 creating better dashboardmyth reality creating better dashboardmyth orreality beta version 20 built using dash plotly instead matplotlib radmila feb 16 weekend ai project making visual assistant people vision impairments weekend ai project making visual assistant people vision impairments running multimodal llava model camera speech synthesis dmitrii eliuseev feb 16 exploring object detection rcnn modelsa comprehensive beginners guide part 2 exploring object detection rcnn modelsa comprehensive beginners guide part2 object detection models raghav bali feb 16 power geospatial intelligence similarity analysis data mapping power geospatial intelligence similarity analysis datamapping strategically enhancing address mapping data integration using geocoding stringmatching kirsten jiayi pan feb 16 emotionsintheloop emotionsintheloop analyzing life scanned jane tea mustac feb 16 seamless indepth walkthrough metas new opensource suite translation models seamless indepth walkthrough metas new opensource suite translation models metas opensource seamless models deep dive translation model architectures implementation guide using huggingface luis roque feb 16 microservices vs monolithic approaches data microservices vs monolithic approaches indata microservice vs monolith debate rages software reduced gentle simmer dataworld hugo lu feb 16 towards data science latest stories archive medium\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def retrieve_top_n_similar_documents(query, inverted_index, data, vectorizer, docvectors, top_n=5):\n",
    "    cleaned_query = text_preprocess(query)\n",
    "    query_vector = vectorizer.transform([cleaned_query])\n",
    "\n",
    "    similarity_scores = cosine_similarity(query_vector, docvectors)\n",
    "    sorted_indices = np.argsort(similarity_scores[0])[::-1][:top_n]\n",
    "\n",
    "    top_n_documents = [data[idx] for idx in sorted_indices]\n",
    "\n",
    "    return top_n_documents\n",
    "\n",
    "# Example usage:\n",
    "query = \"dta science\"\n",
    "top_n_similar_documents = retrieve_top_n_similar_documents(query, inverted_index, data['text'], tfidf_vectorizer, docvectors, top_n=3)\n",
    "print(\"Top 3 similar documents:\")\n",
    "for i, doc in enumerate(top_n_similar_documents, 1):\n",
    "    print(f\"Document {i}: {doc}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOJ1F5WVlKbF4KYk3IVVJoc",
   "collapsed_sections": [
    "3aw-jpgZgmMn"
   ],
   "mount_file_id": "1uq4MtodJco9sNZOgQtkLR87wJbLod8h9",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
