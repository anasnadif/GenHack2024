{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7721043,"sourceType":"datasetVersion","datasetId":4510098},{"sourceId":7721241,"sourceType":"datasetVersion","datasetId":4510254},{"sourceId":7724004,"sourceType":"datasetVersion","datasetId":4512277}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"toc_visible":true}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["!pip install langchain\n","!pip install accelerate\n","!pip install pyvis\n","!pip install transformers\n","!python3 -m pip install googlesearch-python\n","!pip install youtube-search\n","\n","import pandas as pd\n","import numpy as np\n","import itertools\n","\n","import json\n","import networkx as nx\n","import requests\n","\n","from transformers import AutoTokenizer, AutoModel\n","import torch\n","import torch.nn.functional as F\n","from transformers import pipeline\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","from googlesearch import search\n","\n","from tqdm import tqdm\n","tqdm.pandas()"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2024-02-29T16:29:49.071040Z","iopub.execute_input":"2024-02-29T16:29:49.072134Z","iopub.status.idle":"2024-02-29T16:31:40.222317Z","shell.execute_reply.started":"2024-02-29T16:29:49.072088Z","shell.execute_reply":"2024-02-29T16:31:40.221442Z"},"trusted":true,"id":"RIgX2Slw9-2A","outputId":"4dc25bd2-8f3e-44cc-c1ab-79cabcad9f36"},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting langchain\n  Downloading langchain-0.1.9-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.33)\nCollecting langchain-community<0.1,>=0.0.21 (from langchain)\n  Downloading langchain_community-0.0.24-py3-none-any.whl.metadata (8.1 kB)\nCollecting langchain-core<0.2,>=0.1.26 (from langchain)\n  Downloading langchain_core-0.1.27-py3-none-any.whl.metadata (6.0 kB)\nCollecting langsmith<0.2.0,>=0.1.0 (from langchain)\n  Downloading langsmith-0.1.10-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.24.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.26->langchain) (4.2.0)\nCollecting packaging<24.0,>=23.2 (from langchain-core<0.2,>=0.1.26->langchain)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain)\n  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m775.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2023.11.17)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.2.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nDownloading langchain-0.1.9-py3-none-any.whl (816 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.0/817.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading langchain_community-0.0.24-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.27-py3-none-any.whl (250 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.8/250.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langsmith-0.1.10-py3-none-any.whl (63 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, orjson, langsmith, langchain-core, langchain-community, langchain\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.0.11 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.8.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.1.9 langchain-community-0.0.24 langchain-core-0.1.27 langsmith-0.1.10 orjson-3.9.15 packaging-23.2\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.26.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (23.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nCollecting pyvis\n  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: ipython>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from pyvis) (8.20.0)\nRequirement already satisfied: jinja2>=2.9.6 in /opt/conda/lib/python3.10/site-packages (from pyvis) (3.1.2)\nCollecting jsonpickle>=1.4.1 (from pyvis)\n  Downloading jsonpickle-3.0.3-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: networkx>=1.11 in /opt/conda/lib/python3.10/site-packages (from pyvis) (3.2.1)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (0.1.6)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (3.0.42)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (2.17.2)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (0.6.2)\nRequirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (5.9.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (4.8.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=2.9.6->pyvis) (2.1.3)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis) (0.2.13)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.2)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=5.3.0->pyvis) (1.16.0)\nDownloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hDownloading jsonpickle-3.0.3-py3-none-any.whl (40 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: jsonpickle, pyvis\nSuccessfully installed jsonpickle-3.0.3 pyvis-0.3.2\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.37.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nCollecting googlesearch-python\n  Downloading googlesearch-python-1.2.3.tar.gz (3.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.9 in /opt/conda/lib/python3.10/site-packages (from googlesearch-python) (4.12.2)\nRequirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.10/site-packages (from googlesearch-python) (2.31.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->googlesearch-python) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->googlesearch-python) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->googlesearch-python) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->googlesearch-python) (2023.11.17)\nBuilding wheels for collected packages: googlesearch-python\n  Building wheel for googlesearch-python (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for googlesearch-python: filename=googlesearch_python-1.2.3-py3-none-any.whl size=4209 sha256=65ed47c6abde61b0e6cc706830b40fe3c8ba857432693bdd63c07a96da2f1aa5\n  Stored in directory: /root/.cache/pip/wheels/98/24/e9/6c225502948c629b01cc895f86406819281ef0da385f3eb669\nSuccessfully built googlesearch-python\nInstalling collected packages: googlesearch-python\nSuccessfully installed googlesearch-python-1.2.3\nCollecting youtube-search\n  Downloading youtube_search-2.1.2-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from youtube-search) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-search) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-search) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-search) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->youtube-search) (2023.11.17)\nDownloading youtube_search-2.1.2-py3-none-any.whl (3.4 kB)\nInstalling collected packages: youtube-search\nSuccessfully installed youtube-search-2.1.2\n","output_type":"stream"},{"name":"stderr","text":"2024-02-29 16:31:26.501905: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-29 16:31:26.502033: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-29 16:31:26.660590: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":["torch.backends.cuda.enable_mem_efficient_sdp(False)\n","torch.backends.cuda.enable_flash_sdp(False)"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:31:40.223913Z","iopub.execute_input":"2024-02-29T16:31:40.224499Z","iopub.status.idle":"2024-02-29T16:31:40.229373Z","shell.execute_reply.started":"2024-02-29T16:31:40.224471Z","shell.execute_reply":"2024-02-29T16:31:40.228312Z"},"trusted":true,"id":"fLe-J-qw9-2J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# if having an openai api\n","!pip install --upgrade openai"],"metadata":{"execution":{"iopub.status.busy":"2024-03-05T08:19:39.678841Z","iopub.execute_input":"2024-03-05T08:19:39.679093Z","iopub.status.idle":"2024-03-05T08:19:55.126071Z","shell.execute_reply.started":"2024-03-05T08:19:39.679070Z","shell.execute_reply":"2024-03-05T08:19:55.124980Z"},"trusted":true,"id":"2Snk073U9-2O","outputId":"e0bf3ea3-b286-4059-f01a-3bc72231aece"},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting openai\n  Downloading openai-1.13.3-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\nCollecting httpx<1,>=0.23.0 (from openai)\n  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (2.5.3)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.0)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.1)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from openai) (4.9.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\nCollecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\nDownloading openai-1.13.3-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: httpcore, httpx, openai\nSuccessfully installed httpcore-1.0.4 httpx-0.27.0 openai-1.13.3\n","output_type":"stream"}]},{"cell_type":"code","source":["from openai import OpenAI\n","\n","OPENAI_API_KEY=\"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\"\n","client = OpenAI(api_key = OPENAI_API_KEY)"],"metadata":{"execution":{"iopub.status.busy":"2024-03-05T08:41:00.682304Z","iopub.execute_input":"2024-03-05T08:41:00.682901Z","iopub.status.idle":"2024-03-05T08:41:01.542774Z","shell.execute_reply.started":"2024-03-05T08:41:00.682856Z","shell.execute_reply":"2024-03-05T08:41:01.541902Z"},"trusted":true,"id":"6NGIbCCp9-2P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generating the Knowledge Graph"],"metadata":{"id":"J_falsWx9-2Q"}},{"cell_type":"markdown","source":["## Using OpenAI"],"metadata":{"id":"vfVCaVt49-2T"}},{"cell_type":"code","source":["SYS_PROMPT = (\n","        \"You are a knowledge graph generator tasked with summarizing and extracting key terms and their relationships from a given text.\"\n","        \"Given a text snippet (delimited by ```), your initial objective is to summarize the text, retaining only the essential facts.\"\n","        \"Subsequently, identify the significant terms within the summary.\"\n","            \"\\tTerms may encompass objects, entities, locations, organizations, individuals,\"\n","            \"\\tconditions, acronyms, documents, services, concepts, etc.\"\n","            \"\\tTerms should be granular for precise representation.\\n\\n\"\n","        \"Determine potential relationships between identified terms.\"\n","            \"\\tTerms occurring in the same sentence or paragraph often share connections.\"\n","            \"\\tTerms can have multiple associations with other terms.\\n\\n\"\n","        \"Define the relationship between each pair of related terms.\\n\\n\"\n","        \"Format your output as a JSON list. Each element contains a pair of terms\"\n","        \"and their corresponding relationship, as illustrated below:\"\n","        \"[\\n\"\n","        \"   {\\n\"\n","        '       \"node_1\": \"A concept from the extracted ontology\",\\n'\n","        '       \"node_2\": \"A related concept from the extracted ontology\",\\n'\n","        '       \"edge\": \"Relationship between the two concepts, concept_1 and concept_2\"\\n'\n","        \"   }, {...}\\n\"\n","        \"]\"\n","    )\n","\n","def Graph_prompt(chunk):\n","    i=0\n","    while True:\n","        try:\n","            graph = client.chat.completions.create(\n","              model=\"gpt-4-0125-preview\",\n","              messages = [\n","                      {\n","                          \"role\": \"system\",\n","                          \"content\": SYS_PROMPT,\n","                      },\n","                      {\n","                          \"role\": \"user\",\n","                          \"content\": f\"context: ```{chunk}``` \\n\\n output: \"\n","                      }\n","                  ]\n","            )\n","            result = graph.choices[0].message.content\n","            start_index = result.find('[\\n')\n","            end_index = result.find(']')\n","            if end_index == -1: end_index = len(result) -1\n","            outputs = result[start_index:end_index+1].replace('\\n', '')\n","            print(outputs)\n","\n","            graph_out = json.loads(outputs)\n","            result = [dict(item) for item in graph_out]\n","            break\n","\n","        except Exception as e:\n","            print(f\"An error occurred: {e}\")\n","            print(\"Attempting to rerun the code...\")\n","            if i > 10:\n","                print('Fatal too many errors')\n","                break\n","            i+=1\n","            continue\n","    return graph_out"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T15:48:05.778511Z","iopub.execute_input":"2024-02-29T15:48:05.779018Z","iopub.status.idle":"2024-02-29T15:48:05.792730Z","shell.execute_reply.started":"2024-02-29T15:48:05.778982Z","shell.execute_reply":"2024-02-29T15:48:05.791603Z"},"trusted":true,"id":"vBP6nqCD9-2U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Using Zephyr (similar performance) instead of Openai"],"metadata":{"id":"wQXUdNGG9-2W"}},{"cell_type":"code","source":["pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")"],"metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-02-29T16:31:40.230531Z","iopub.execute_input":"2024-02-29T16:31:40.230870Z","iopub.status.idle":"2024-02-29T16:33:46.325142Z","shell.execute_reply.started":"2024-02-29T16:31:40.230843Z","shell.execute_reply":"2024-02-29T16:33:46.324334Z"},"trusted":true,"colab":{"referenced_widgets":["3a497c60cea74b33b6ec19482d4c4710","57dda9c067774f31b01c46298fecc430","f13b40564ba74be683f521521f249fd0","1ddccc4e92c2477dbe46a2e42135fd21","ed2a507aa792436487fd9b9691132c2a","dcb46c2088084161850dc7b4a82d5621","26cfe7cc89604144825b4bac403b7a7b","dd045b9e3c7b480db8da669890dc60ab","59be050448af43ffa666cb3c15f1fc9c","58a44a06700d460bbbb133310aaa5f2c","8fc8d93795254dbd9a94d05c09155126","3b52ca5d678c44fa96a241497d90247b","d7cfc872e76d49c9947394bcd988e4db","a254dc4063454218bbc0e6e5557110b4","d887f357500a46deb26f24555880b52f","5aae97d29cf04e0588d114f596ae7d3e","7038b9bf99404f869862adc7e7a507c4","27cd5b2d07b344fcb13a16c9385aac12"]},"id":"4n6N98Xn9-2X","outputId":"84235310-0f50-4a84-99d8-75693eeceff9"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a497c60cea74b33b6ec19482d4c4710"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57dda9c067774f31b01c46298fecc430"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f13b40564ba74be683f521521f249fd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ddccc4e92c2477dbe46a2e42135fd21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed2a507aa792436487fd9b9691132c2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcb46c2088084161850dc7b4a82d5621"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26cfe7cc89604144825b4bac403b7a7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd045b9e3c7b480db8da669890dc60ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59be050448af43ffa666cb3c15f1fc9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58a44a06700d460bbbb133310aaa5f2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fc8d93795254dbd9a94d05c09155126"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b52ca5d678c44fa96a241497d90247b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7cfc872e76d49c9947394bcd988e4db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a254dc4063454218bbc0e6e5557110b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d887f357500a46deb26f24555880b52f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5aae97d29cf04e0588d114f596ae7d3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7038b9bf99404f869862adc7e7a507c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27cd5b2d07b344fcb13a16c9385aac12"}},"metadata":{}}]},{"cell_type":"code","source":["SYS_PROMPT = (\n","        \"You are a knowledge graph generator tasked with summarizing and extracting key terms and their relationships from a given text.\"\n","        \"Given a text snippet (delimited by ```), your initial objective is to summarize the text, retaining only the essential facts.\"\n","        \"Subsequently, identify the significant terms within the summary.\"\n","            \"\\tTerms may encompass objects, entities, locations, organizations, individuals,\"\n","            \"\\tconditions, acronyms, documents, services, concepts, etc.\"\n","            \"\\tTerms should be granular for precise representation.\\n\\n\"\n","        \"Determine potential relationships between identified terms.\"\n","            \"\\tTerms occurring in the same sentence or paragraph often share connections.\"\n","            \"\\tTerms can have multiple associations with other terms.\\n\\n\"\n","        \"Define the relationship between each pair of related terms.\\n\\n\"\n","        \"Format your output as a JSON list. Each element contains a pair of terms\"\n","        \"and their corresponding relationship, as illustrated below:\"\n","        \"[\\n\"\n","        \"   {\\n\"\n","        '       \"node_1\": \"A concept from the extracted ontology\",\\n'\n","        '       \"node_2\": \"A related concept from the extracted ontology\",\\n'\n","        '       \"edge\": \"Relationship between the two concepts, node_1 and node_2\"\\n'\n","        \"   }, {...}\\n\"\n","        \"]\"\n","    )\n","\n","def Graph_prompt(chunk):\n","    messages = [\n","          {\n","              \"role\": \"system\",\n","              \"content\": SYS_PROMPT,\n","          },\n","          {\n","              \"role\": \"user\",\n","              \"content\": f\"context: ```{chunk}``` \\n\\n output: \"\n","          },\n","      ]\n","\n","    i=0\n","    while True:\n","        try:\n","            prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","            outputs = pipe(prompt, max_new_tokens=2048, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)[0][\"generated_text\"]\n","\n","            start = '|assistant|>\\n'\n","            start_index = outputs.find(start)\n","            inter = outputs[start_index + len(start):]\n","\n","            start_index = inter.find('[\\n')\n","            end_index = inter.find(']')\n","            if end_index == -1: end_index = len(inter) -1\n","            outputs = inter[start_index:end_index+1].replace('\\n', '')\n","            print(outputs)\n","\n","            graph_out = json.loads(outputs)\n","            result = [dict(item) for item in graph_out]\n","            break  # Exit the loop if execution is successful\n","\n","        except Exception as e:\n","            print(f\"An error occurred: {e}\")\n","            print(\"Attempting to rerun the code...\")\n","            if i > 10:\n","                print('Fatal too many errors')\n","                break\n","            i+=1\n","            continue\n","    return graph_out"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:33:46.327106Z","iopub.execute_input":"2024-02-29T16:33:46.327418Z","iopub.status.idle":"2024-02-29T16:33:46.338725Z","shell.execute_reply.started":"2024-02-29T16:33:46.327393Z","shell.execute_reply":"2024-02-29T16:33:46.337861Z"},"trusted":true,"id":"5rLIbSHS9-2Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Getting Similarities to add 'Related to' edges to the KG"],"metadata":{"id":"nK8Ij9Gt9-2b"}},{"cell_type":"code","source":["#Mean Pooling - Take attention mask into account for correct averaging\n","def mean_pooling(model_output, attention_mask):\n","    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n","    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","\n","# Load model from HuggingFace Hub\n","tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n","model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')"],"metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-02-29T16:33:46.340181Z","iopub.execute_input":"2024-02-29T16:33:46.340666Z","iopub.status.idle":"2024-02-29T16:33:48.545693Z","shell.execute_reply.started":"2024-02-29T16:33:46.340633Z","shell.execute_reply":"2024-02-29T16:33:48.544781Z"},"trusted":true,"colab":{"referenced_widgets":["844b22b222ef4f6da41cebf9b94755bd","c31d1c0d7e284566bc6b361cd2be1aac","b5c41991946146c59a4281bc5818727e","9655eb3605ac409a842228a4b0673d86","ecdd2c12d52346f2984d80b5d2e74e5a","71c90155a3444f01b6e409d78d1b7bde"]},"id":"i-5YYJeT9-2d","outputId":"d685be83-4b4b-4cca-8589-9f648840b9d1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"844b22b222ef4f6da41cebf9b94755bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c31d1c0d7e284566bc6b361cd2be1aac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5c41991946146c59a4281bc5818727e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9655eb3605ac409a842228a4b0673d86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecdd2c12d52346f2984d80b5d2e74e5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71c90155a3444f01b6e409d78d1b7bde"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"code","source":["def sim(nodes):\n","    try:\n","        encoded_input = tokenizer(nodes, padding=True, return_tensors='pt')\n","\n","        with torch.no_grad():\n","            model_output = model(**encoded_input)\n","\n","        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n","\n","    except:\n","        return 0\n","\n","    return F.cosine_similarity(sentence_embeddings[0].unsqueeze(0), sentence_embeddings[1].unsqueeze(0)).item()"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:33:48.546872Z","iopub.execute_input":"2024-02-29T16:33:48.547202Z","iopub.status.idle":"2024-02-29T16:33:48.553211Z","shell.execute_reply.started":"2024-02-29T16:33:48.547164Z","shell.execute_reply":"2024-02-29T16:33:48.552266Z"},"trusted":true,"id":"Z8eTy21P9-2f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Notes to work On"],"metadata":{"id":"V5FOMcDO9-2g"}},{"cell_type":"code","source":["list_text = [\"\"\"Course: Introduction to Data Science\n","\n","Week 1: Foundations of Data Science\n","\n","Understanding Data Science:\n","\n","Definition: Data Science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract insights and knowledge from structured and unstructured data.\n","Importance: Data science helps in making informed decisions, predicting trends, and gaining valuable insights from large datasets.\n","Key Concepts:\n","\n","Data Types: Different types of data - numerical, categorical, and text data.\n","Data Sources: Learning about various sources of data, including databases, APIs, and web scraping.\n","Data Exploration: Techniques for exploring and understanding datasets using descriptive statistics and visualizations.\n","Week 2: Data Cleaning and Preprocessing\n","\n","Data Cleaning:\n","\n","Importance: Clean data is essential for accurate analysis and modeling.\n","Techniques: Handling missing values, removing duplicates, and addressing outliers.\n","Data Preprocessing:\n","\n","Feature Engineering: Creating new features from existing ones to improve model performance.\n","Scaling and Normalization: Standardizing features to ensure equal importance in the modeling process.\n","Week 3: Exploratory Data Analysis (EDA)\n","\n","EDA Techniques:\n","\n","Univariate Analysis: Examining a single variable to understand its distribution.\n","Bivariate Analysis: Analyzing relationships between two variables.\n","Multivariate Analysis: Exploring relationships among multiple variables simultaneously.\n","Visualization Tools:\n","\n","Matplotlib and Seaborn: Creating static visualizations for data exploration.\n","Plotly: Generating interactive and dynamic visualizations.\n","Week 4: Statistical Analysis for Data Science\n","\n","Descriptive Statistics:\n","\n","Measures of Central Tendency: Mean, median, mode.\n","Measures of Dispersion: Range, variance, standard deviation.\n","Inferential Statistics:\n","\n","Hypothesis Testing: Understanding the basics of hypothesis testing and p-values.\n","Confidence Intervals: Estimating the range within which a population parameter is likely to fall.\n","Week 5: Machine Learning Fundamentals\n","\n","Supervised Learning:\n","\n","Classification vs. Regression: Differentiating between predicting categories and predicting numerical values.\n","Algorithms: Introduction to linear regression, logistic regression, decision trees, and random forests.\n","Unsupervised Learning:\n","\n","Clustering: Grouping similar data points together using algorithms like K-means clustering.\n","Dimensionality Reduction: Techniques like Principal Component Analysis (PCA).\n","Week 6: Model Evaluation and Selection\n","\n","Evaluation Metrics:\n","\n","Accuracy, precision, recall, F1-score: Metrics for assessing model performance.\n","Confusion Matrix: Visualizing the performance of a classification model.\n","Cross-Validation:\n","\n","Techniques like k-fold cross-validation to assess how well a model generalizes to new data.\n","Week 7: Big Data and Tools\n","\n","Introduction to Big Data:\n","\n","Definition and characteristics of big data.\n","Challenges in processing and analyzing large datasets.\n","Big Data Tools:\n","\n","Apache Hadoop and Apache Spark: Understanding distributed computing frameworks for big data processing.\n","Week 8: Capstone Project\n","\n","Applying the learned concepts to a real-world problem.\n","Developing a complete data science pipeline from data collection to model deployment.\n","Presentation and documentation of the project\n","\"\"\",\n","'''\n","Data Visualization Process/Workflow\n","The data visualization process or workflow includes the fowling key steps.\n","\n","1. Develop your research question\n","This may be a business problem or any other related problem that could be solved with a data-driven approach. You should note all the objectives and outcomes plus required resources such as datasets, open-source software libraries, etc.\n","\n","2. Get or create your data\n","The next step is collecting data. You can use existing datasets if they’re relevant to your research question. Alternatively, you can download open-source datasets from the internet or do web scraping to collect data.\n","\n","3. Clean your data\n","Real-world data are messy. So, you need to clean them before using them for visualization. You can identify missing values and outliers and treat them accordingly. You can perform feature selection and remove unnecessary features from the data. You can create a new set of features based on the original features.\n","\n","4. Choose a chart type\n","The chart type depends on many factors. For example, it depends on the feature type (numerical or categorical). It also depends on the type of visualization you need. Let’s say you have two numerical features. If you want to find their distributions, you can create two histograms for each feature. If you want to plot their variations, you can create box and whisker plots for each feature. You can create a scatterplot if you want to find a relationship (linear or non-linear, positive or negative) between the two features.\n","\n","5. Choose your tool\n","You can use open-source data visualization tools such as matplotlib, seaborn, plotty and ggplot. You can also use API-based software such as Matlab, Minitab, SPSS, etc.\n","\n","6. Prepare data\n","You can extract relevant features. You can do feature standardization if the values of the features are not on the same scale. You can apply data preprocessing steps such as PCA to reduce the dimensionality of the data. That will allow you to visualize high-dimensional data in 2D and 3D plots!\n","\n","7. Create a chart\n","This is the final step. Here. You define the title and names for the axes. You should also choose a proper chart background to ensure the content is easily readable.\n","\n","Tools and Software for Data Visualization\n","There are multiple tools and software available for data visualization.\n","\n","1. Python provides open-source libraries such as\n","\n","Matplotlib\n","Seaborn\n","Plotty\n","Bokeh\n","Altair\n","2. R provides open-source libraries such as\n","\n","Ggplot2\n","Lattice\n","3. Other data visualization libraries\n","\n","IBM SPSS\n","Minitab\n","Matlab for data visualization\n","Tableau\n","Microsoft Power BI are popular among data scientists.\n","Tableau and Microsoft Power BI are popular among data scientists.''',\n","            '''Albert Einstein (/ˈaɪnstaɪn/ EYEN-styne;[4] German: [ˈalbɛɐt ˈʔaɪnʃtaɪn] ⓘ; 14 March 1879 – 18 April 1955) was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time. Best known for developing the theory of relativity, Einstein also made important contributions to quantum mechanics, and was thus a central figure in the revolutionary reshaping of the scientific understanding of nature that modern physics accomplished in the first decades of the twentieth century.[1][5] His mass–energy equivalence formula E = mc2, which arises from relativity theory, has been called \"the world's most famous equation\".[6] He received the 1921 Nobel Prize in Physics \"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\",[7] a pivotal step in the development of quantum theory. His work is also known for its influence on the philosophy of science.[8][9] In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time.[10] His intellectual achievements and originality have made the word Einstein broadly synonymous with genius.[11]\n","\n","Born in the German Empire, Einstein moved to Switzerland in 1895, forsaking his German citizenship (as a subject of the Kingdom of Württemberg)[note 1] the following year. In 1897, at the age of seventeen, he enrolled in the mathematics and physics teaching diploma program at the Swiss federal polytechnic school in Zürich, graduating in 1900. In 1901, he acquired Swiss citizenship, which he kept for the rest of his life. In 1903, he secured a permanent position at the Swiss Patent Office in Bern. In 1905, he submitted a successful PhD dissertation to the University of Zurich. In 1914, he moved to Berlin in order to join the Prussian Academy of Sciences and the Humboldt University of Berlin. In 1917, he became director of the Kaiser Wilhelm Institute for Physics; he also became a German citizen again, this time as a subject of the Kingdom of Prussia.[note 1]\n","\n","In 1933, while he was visiting the United States, Adolf Hitler came to power in Germany. Horrified by the Nazi \"war of extermination\" against his fellow Jews,[12] Einstein decided to remain in the US, and was granted American citizenship in 1940.[13] On the eve of World War II, he endorsed a letter to President Franklin D. Roosevelt alerting him to the potential German nuclear weapons program and recommending that the US begin similar research. Einstein supported the Allies but generally viewed the idea of nuclear weapons with great dismay.[14]\n","\n","In 1905, sometimes described as his annus mirabilis (miracle year), Einstein published four groundbreaking papers.[15] These outlined a theory of the photoelectric effect, explained Brownian motion, introduced his special theory of relativity—a theory which addressed the inability of classical mechanics to account satisfactorily for the behavior of the electromagnetic field—and demonstrated that if the special theory is correct, mass and energy are equivalent to each other. In 1915, he proposed a general theory of relativity that extended his system of mechanics to incorporate gravitation. A cosmological paper that he published the following year laid out the implications of general relativity for the modeling of the structure and evolution of the universe as a whole.[16][17] The middle part of his career also saw him making important contributions to statistical mechanics and quantum theory. Especially notable was his work on the quantum physics of radiation, in which light consists of particles, subsequently called photons. For much of the last phase of his academic life, Einstein worked on two endeavors that proved ultimately unsuccessful. Firstly, he advocated against quantum theory's introduction of fundamental randomness into science's picture of the world, objecting that \"God does not play dice\".[18] Secondly, he attempted to devise a unified field theory by generalizing his geometric theory of gravitation to include electromagnetism too. As a result, he became increasingly isolated from the mainstream of modern physics.'''\n",",\n","'''Cuisine Enthusiast's Notebook\n","\n","Recipe Creations:\n","\n","Mango Avocado Salad:\n","\n","Ripe mango chunks, diced avocado, cherry tomatoes.\n","Drizzle with lime vinaigrette, sprinkle with cilantro.\n","Optional: Add grilled shrimp for protein.\n","Homemade Pasta:\n","\n","Fresh pasta dough with a mix of semolina and all-purpose flour.\n","Tomato basil sauce with garlic and a touch of red pepper flakes.\n","Garnish with grated Parmesan and fresh basil.\n","Chocolate Fondue:\n","\n","Dark chocolate, heavy cream, and a splash of Grand Marnier.\n","Dippables: Strawberries, banana slices, marshmallows, and pretzels.\n","Cooking Techniques:\n","\n","Sautéing vs. Stir-Frying:\n","\n","Sautéing: Quick cooking over medium-high heat with a small amount of oil.\n","Stir-Frying: High-heat cooking with constant stirring, often in a wok.\n","Roasting and Grilling:\n","\n","Roasting: Dry heat in the oven for caramelization and flavor development.\n","Grilling: Direct heat with the smoky essence, perfect for meats and vegetables.\n","Knife Skills:\n","\n","Mastering basic cuts – julienne, chiffonade, and brunoise.\n","Proper knife grip and honing for precision.\n","Ingredient Exploration:\n","\n","Herb Garden Essentials:\n","\n","Basil, rosemary, thyme, and mint for fresh, aromatic flavors.\n","Tips on growing herbs at home for a constant supply.\n","Exotic Spices:\n","\n","Garam masala, smoked paprika, and sumac for global culinary exploration.\n","Creating spice blends for personalized seasoning.\n","Specialty Oils and Vinegars:\n","\n","Truffle oil, balsamic vinegar, and chili-infused oils.\n","Enhancing dishes with unique and nuanced flavors.\n","Tasting Notes:\n","\n","Wine Pairing Adventures:\n","\n","Learning to pair reds and whites with different cuisines.\n","Notes on the synergy between wine and food flavors.\n","Cheese Tasting Journal:\n","\n","Exploring artisanal cheeses from local dairies.\n","Pairing cheese varieties with fruits, nuts, and crackers.\n","Chocolate Varieties:\n","\n","Dark, milk, and white chocolate tasting experiences.\n","Recognizing flavor notes and cocoa percentages.\n","Foodie Adventures:\n","\n","Restaurant Reviews:\n","\n","Hidden gems and fine dining experiences.\n","Capturing ambiance, service, and standout dishes.\n","Market Finds:\n","\n","Unusual produce and specialty items discovered at local markets.\n","Incorporating unique ingredients into home-cooked meals.\n","Upcoming Culinary Goals:\n","\n","Mastering the art of French pastry.\n","Perfecting the technique of making sushi at home.\n","Hosting themed dinner parties showcasing diverse cuisines.''',\n","'''The image shows a diagram of the Transformer model architecture, which is a type of neural network primarily used in the field of natural language processing (NLP). The Transformer model is known for its effectiveness in sequence-to-sequence tasks such as language translation.\n","\n","The architecture consists of two main parts: the encoder on the left and the decoder on the right. Both the encoder and decoder are composed of a stack of identical layers, which is indicated by \"Nx,\" suggesting that there are multiple layers stacked on top of each other.\n","\n","The encoder consists of:\n","- Input Embedding: A layer that converts the input tokens into vectors (embeddings).\n","- Positional Encoding: A layer that adds positional information to the input embeddings, allowing the model to use the order of the tokens.\n","- Multi-Head Attention: A mechanism that allows the model to focus on different parts of the input sequence for better understanding.\n","- Add & Norm: Layers that apply residual connections and layer normalization.\n","- Feed Forward: A fully connected feed-forward network that transforms the attention output.\n","\n","The decoder includes similar components along with an additional component for attention:\n","- Output Embedding: Converts the target tokens into vectors.\n","- Positional Encoding: Similar to the encoder, adding positional information to the target embeddings.\n","- Masked Multi-Head Attention: A mechanism that prevents future tokens from being accessed, ensuring that the prediction for a certain position relies only on the known outputs.\n","- Multi-Head Attention: Attention that considers the output of the encoder''',\n","'''The image presents a schematic illustration of the Vision Transformer (ViT) architecture, which is a type of artificial intelligence model used for image processing tasks. The process starts with an input image that gets divided into a series of smaller image patches. These patches are then flattened and linearly projected into a sequence of vectors (embedded patches). Additionally, position embeddings are added to these vectors to maintain positional information.\n","\n","The sequence of vectors, along with an extra learnable class embedding, is passed through multiple layers of the transformer encoder. Each transformer encoder layer is composed of multi-head self-attention mechanisms and multilayer perceptrons (MLPs), with normalization (Norm) layers in between. The attention mechanism allows the model to focus on different parts of the image when making decisions.\n","\n","After processing the sequence through the transformer encoder, the class embedding (which is initially arbitrary and learns to represent the whole image during training) is used by a MLP head to predict the class of the image (e.g., bird, ball, car, etc.).\n","\n","On the right side of the diagram, there's a more detailed illustration of the inner workings of a transformer encoder layer, showing the flow of data through the multi-head attention and MLP, as well as the residual connections around them.'''\n","]"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:33:48.554622Z","iopub.execute_input":"2024-02-29T16:33:48.554919Z","iopub.status.idle":"2024-02-29T16:33:48.574636Z","shell.execute_reply.started":"2024-02-29T16:33:48.554897Z","shell.execute_reply":"2024-02-29T16:33:48.573817Z"},"trusted":true,"id":"U0vCSuki9-2h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = '\\n'.join([tx['summary_text'] for tx in text_sum])"],"metadata":{"execution":{"iopub.status.busy":"2024-02-23T13:42:42.694703Z","iopub.status.idle":"2024-02-23T13:42:42.695050Z","shell.execute_reply.started":"2024-02-23T13:42:42.694880Z","shell.execute_reply":"2024-02-23T13:42:42.694895Z"},"trusted":true,"id":"msgwi3PH9-2m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Necessary Functions"],"metadata":{"id":"GSbXVe389-2n"}},{"cell_type":"code","source":["splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=750,\n","    chunk_overlap=75,\n","    length_function=len,\n","    is_separator_regex=False,\n",")\n","\n","def text_to_graph(text, Community_KeyNodes):\n","    chunks = splitter.split_text(text)\n","    df_text = {'text': chunks,\n","               'id_chunk': range(len(chunks))}\n","    df_text = pd.DataFrame(df_text)\n","\n","    results = df_text.progress_apply(lambda row: Graph_prompt(row.text), axis=1)\n","\n","    transformed_data = []\n","    for list0 in results:\n","        for sublist in list0:\n","            if 'edge' in sublist.keys() and 'node_1' in sublist.keys() and 'node_2' in sublist.keys():\n","                transformed_data.append((sublist['node_1'].lower(), sublist['edge'].lower(), sublist['node_2'].lower()))\n","\n","    df_KG = pd.DataFrame(transformed_data, columns=['node1', 'edge', 'node2'])\n","    # Making sure there is no empty cells\n","    df_KG.drop(index=np.where(df_KG['node1']==\"\")[0], inplace=True)\n","    df_KG.drop(index=np.where(df_KG['node2']==\"\")[0], inplace=True)\n","    df_KG.drop(index=np.where(df_KG['edge']==\"\")[0], inplace=True)\n","\n","    nodes = set(np.concatenate((np.unique(df_KG['node1']), np.unique(df_KG['node2']))))\n","    if Community_KeyNodes:\n","        KeyNodes = [item for sublist in Community_KeyNodes for item in sublist]\n","        nodes = nodes.union(KeyNodes)\n","\n","    pairs = pd.DataFrame(list(itertools.combinations(nodes, 2)), columns=['node1', 'node2'])\n","    pairs['similarity'] = pairs.progress_apply(lambda row:sim(list(row)), axis=1)\n","    pairs = pairs[pairs['similarity']>0.4]\n","    pairs.insert(1, 'edge', 'Related to')\n","    pairs.drop('similarity', axis=1, inplace=True)\n","    df = pd.concat([df_KG, pairs], axis=0)\n","\n","    G = nx.MultiGraph()\n","    for index, row in df.iterrows():\n","        G.add_edge(\n","            str(row[\"node1\"]),\n","            str(row[\"node2\"]),\n","            title=row[\"edge\"]\n","        )\n","    return G"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:33:48.575814Z","iopub.execute_input":"2024-02-29T16:33:48.576583Z","iopub.status.idle":"2024-02-29T16:33:48.591203Z","shell.execute_reply.started":"2024-02-29T16:33:48.576551Z","shell.execute_reply":"2024-02-29T16:33:48.590352Z"},"trusted":true,"id":"KCgfjLOi9-2o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_nodes_with_highest_degrees(graph, nodes, k=4):\n","    degrees = {node: len(graph[node]) for node in nodes}\n","    sorted_nodes = sorted(degrees, key=degrees.get, reverse=True)\n","    top_k_nodes = sorted_nodes[:k]\n","    return top_k_nodes"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:33:48.592295Z","iopub.execute_input":"2024-02-29T16:33:48.592612Z","iopub.status.idle":"2024-02-29T16:33:48.603525Z","shell.execute_reply.started":"2024-02-29T16:33:48.592581Z","shell.execute_reply":"2024-02-29T16:33:48.602671Z"},"trusted":true,"id":"arkCCkVW9-2p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_communities(KG):\n","    communities_generator = nx.community.girvan_newman(KG)\n","    top_level_communities = next(communities_generator)\n","    try:\n","        top_level_communities = next(communities_generator)\n","    except:\n","        pass\n","    communities = sorted(map(sorted, top_level_communities))\n","    return communities"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:33:48.606055Z","iopub.execute_input":"2024-02-29T16:33:48.606348Z","iopub.status.idle":"2024-02-29T16:33:48.617548Z","shell.execute_reply.started":"2024-02-29T16:33:48.606325Z","shell.execute_reply":"2024-02-29T16:33:48.616723Z"},"trusted":true,"id":"pfr9Ubxh9-2p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_subcommunities(KG, nodes):\n","    flag = True\n","    qu = [nodes]\n","    sub = []\n","\n","    i = 0\n","    while qu and i<1000:\n","        test = get_communities(KG.subgraph(qu[-1]))\n","        if 1 in [len(sub_test) for sub_test in test]:\n","            sub.append(qu[-1])\n","            qu.pop()\n","            i += 1\n","            continue\n","        else:\n","            qu.pop()\n","            qu.extend(test)\n","            i += 1\n","    return sub"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:40:04.379821Z","iopub.execute_input":"2024-02-29T16:40:04.380457Z","iopub.status.idle":"2024-02-29T16:40:04.386849Z","shell.execute_reply.started":"2024-02-29T16:40:04.380425Z","shell.execute_reply":"2024-02-29T16:40:04.385943Z"},"trusted":true,"id":"Mbl6Qyjd9-2q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Knowledge_Graph():\n","    def __init__(self):\n","        self.Community_KeyNodes = []\n","        self.communities = []\n","        self.sub_communities = []\n","        self.KG = nx.MultiGraph()\n","\n","    def process_list_text(self, list_text):\n","        docs_KeyNodes = []\n","        for i, text in enumerate(list_text):\n","            print(f'Processing document number {i+1}...')\n","            text_KG = text_to_graph(text, self.Community_KeyNodes)\n","            docs_KeyNodes.append(get_nodes_with_highest_degrees(text_KG, text_KG.nodes))\n","\n","            self.KG = nx.compose(self.KG, text_KG)\n","\n","            self.communities = get_communities(self.KG)\n","            self.Community_KeyNodes = [get_nodes_with_highest_degrees(self.KG, com) for com in self.communities]\n","\n","        print('Creating Sub_Communities...')\n","        self.sub_communities = []\n","        for com in tqdm(self.communities):\n","            self.sub_communities.append(get_subcommunities(self.KG, com))\n","        return docs_KeyNodes"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:33:48.629834Z","iopub.execute_input":"2024-02-29T16:33:48.630457Z","iopub.status.idle":"2024-02-29T16:33:48.639461Z","shell.execute_reply.started":"2024-02-29T16:33:48.630434Z","shell.execute_reply":"2024-02-29T16:33:48.638695Z"},"trusted":true,"id":"hja0fRsN9-2r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Applying on the Notes"],"metadata":{"id":"gmZCAwOy9-2s"}},{"cell_type":"code","source":["G = Knowledge_Graph()"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:33:48.640362Z","iopub.execute_input":"2024-02-29T16:33:48.640611Z","iopub.status.idle":"2024-02-29T16:33:48.650518Z","shell.execute_reply.started":"2024-02-29T16:33:48.640589Z","shell.execute_reply":"2024-02-29T16:33:48.649554Z"},"trusted":true,"id":"eB6Q8qdm9-2t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["KeyNodes = G.process_list_text(list_text[-2:])"],"metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-02-29T16:33:48.651683Z","iopub.execute_input":"2024-02-29T16:33:48.652002Z","iopub.status.idle":"2024-02-29T16:36:28.239692Z","shell.execute_reply.started":"2024-02-29T16:33:48.651974Z","shell.execute_reply":"2024-02-29T16:36:28.238782Z"},"trusted":true,"id":"bFB6P45N9-2t","outputId":"1c8b212a-362c-4bd3-fdd2-447460006014"},"execution_count":null,"outputs":[{"name":"stdout","text":"Processing document number 1...\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/3 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 67%|██████▋   | 2/3 [00:31<00:15, 15.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[  {    \"node_1\": \"Transformer model\",    \"node_2\": \"neural network\",    \"edge\": \"is a type of\"  },  {    \"node_1\": \"Transformer model\",    \"node_2\": \"natural language processing\",    \"edge\": \"primarily used in the field of\"  },  {    \"node_1\": \"Transformer model\",    \"node_2\": \"sequence-to-sequence tasks\",    \"edge\": \"effectiveness in\"  },  {    \"node_1\": \"Transformer model\",    \"node_2\": \"language translation\",    \"edge\": \"effectiveness in\"  },  {    \"node_1\": \"Transformer model\",    \"node_2\": \"encoder\",    \"edge\": \"consists of\"  },  {    \"node_1\": \"Transformer model\",    \"node_2\": \"decoder\",    \"edge\": \"consists of\"  },  {    \"node_1\": \"Encoder\",    \"node_2\": \"identical layers\",    \"edge\": \"composed of\"  },  {    \"node_1\": \"Decoder\",    \"node_2\": \"identical layers\",    \"edge\": \"composed of\"  },  {    \"node_1\": \"Encoder\",    \"node_2\": \"stack of layers\",    \"edge\": \"indicated by\"  },  {    \"node_1\": \"Decoder\",    \"node_2\": \"stack of layers\",    \"edge\": \"indicated by\"  }]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [00:46<00:00, 15.45s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[  {    \"node_1\": \"Input Embedding\",    \"node_2\": \"Encoder\",    \"edge\": \"Part of\"  },  {    \"node_1\": \"Encoder\",    \"node_2\": \"Positional Encoding\",    \"edge\": \"Contains\"  },  {    \"node_1\": \"Encoder\",    \"node_2\": \"Multi-Head Attention\",    \"edge\": \"Contains\"  },  {    \"node_1\": \"Encoder\",    \"node_2\": \"Add & Norm\",    \"edge\": \"Contains\"  },  {    \"node_1\": \"Encoder\",    \"node_2\": \"Feed Forward\",    \"edge\": \"Contains\"  }]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [01:24<00:00, 28.26s/it]\n","output_type":"stream"},{"name":"stdout","text":"[  {    \"node_1\": \"Masked Multi-Head Attention\",    \"node_2\": \"Attention mechanism\",    \"edge\": \"Specialization relationship\"  },  {    \"node_1\": \"Masked Multi-Head Attention\",    \"node_2\": \"Attention\",    \"edge\": \"Part of relationship\"  },  {    \"node_1\": \"Masked Multi-Head Attention\",    \"node_2\": \"Mechanism\",    \"edge\": \"Part of relationship\"  },  {    \"node_1\": \"Masked Multi-Head Attention\",    \"node_2\": \"Prediction\",    \"edge\": \"Enhances relationship\"  },  {    \"node_1\": \"Masked Multi-Head Attention\",    \"node_2\": \"Target tokens\",    \"edge\": \"Applies to relationship\"  },  {    \"node_1\": \"Positional Encoding\",    \"node_2\": \"Attention\",    \"edge\": \"Part of relationship\"  },  {    \"node_1\": \"Positional Encoding\",    \"node_2\": \"Encoder\",    \"edge\": \"Applies to relationship\"  },  {    \"node_1\": \"Output Embedding\",    \"node_2\": \"Attention\",    \"edge\": \"Part of relationship\"  },  {    \"node_1\": \"Output Embedding\",    \"node_2\": \"Target tokens\",    \"edge\": \"Applies to relationship\"  },  {    \"node_1\": \"Decoder\",    \"node_2\": \"Attention\",    \"edge\": \"Specialization relationship\"  },  {    \"node_1\": \"Decoder\",    \"node_2\": \"Encoder\",    \"edge\": \"Specialization relationship\"  },  {    \"node_1\": \"Decoder\",    \"node_2\": \"Mechanism\",    \"edge\": \"Part of relationship\"  },  {    \"node_1\": \"Decoder\",    \"node_2\": \"Output\",    \"edge\": \"Produces relationship\"  }]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 231/231 [00:03<00:00, 68.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing document number 2...\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/3 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 67%|██████▋   | 2/3 [00:15<00:07,  7.84s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[  {    \"node_1\": \"Vision Transformer (ViT)\",    \"node_2\": \"image processing tasks\",    \"edge\": \"used for\"  },  {    \"node_1\": \"input image\",    \"node_2\": \"Vision Transformer (ViT)\",    \"edge\": \"starts with\"  },  {    \"node_1\": \"image patches\",    \"node_2\": \"input image\",    \"edge\": \"gets divided into\"  },  {    \"node_1\": \"embedded patches\",    \"node_2\": \"image patches\",    \"edge\": \"after\"  },  {    \"node_1\": \"position embeddings\",    \"node_2\": \"embedded patches\",    \"edge\": \"added to\"  }]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [00:37<00:00, 13.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"[  {    \"node_1\": \"sequence of vectors\",    \"node_2\": \"transformer encoder\",    \"edge\": \"composition\"  },  {    \"node_1\": \"transformer encoder\",    \"node_2\": \"multi-head self-attention mechanisms\",    \"edge\": \"part-of\"  },  {    \"node_1\": \"transformer encoder\",    \"node_2\": \"multilayer perceptrons (MLPs)\",    \"edge\": \"part-of\"  },  {    \"node_1\": \"transformer encoder\",    \"node_2\": \"normalization (Norm) layers\",    \"edge\": \"part-of\"  },  {    \"node_1\": \"image\",    \"node_2\": \"sequence of vectors\",    \"edge\": \"transformation\"  },  {    \"node_1\": \"class embedding\",    \"node_2\": \"image\",    \"edge\": \"representation\"  },  {    \"node_1\": \"class embedding\",    \"node_2\": \"MLP head\",    \"edge\": \"input\"  }]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [01:03<00:00, 21.08s/it]\n","output_type":"stream"},{"name":"stdout","text":"[  {    \"node_1\": \"transformer encoder layer\",    \"node_2\": \"multi-head attention\",    \"edge\": \"consists of\"  },  {    \"node_1\": \"transformer encoder layer\",    \"node_2\": \"MLP\",    \"edge\": \"consists of\"  },  {    \"node_1\": \"transformer encoder layer\",    \"node_2\": \"residual connection\",    \"edge\": \"includes\"  },  {    \"node_1\": \"multi-head attention\",    \"node_2\": \"query\",    \"edge\": \"takes\"  },  {    \"node_1\": \"multi-head attention\",    \"node_2\": \"key\",    \"edge\": \"takes\"  },  {    \"node_1\": \"multi-head attention\",    \"node_2\": \"value\",    \"edge\": \"takes\"  },  {    \"node_1\": \"multi-head attention\",    \"node_2\": \"output\",    \"edge\": \"produces\"  },  {    \"node_1\": \"MLP\",    \"node_2\": \"input\",    \"edge\": \"takes\"  },  {    \"node_1\": \"MLP\",    \"node_2\": \"output\",    \"edge\": \"produces\"  }]\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 496/496 [00:07<00:00, 64.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Creating Sub_Communities...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 17.31it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":["communities = G.communities\n","sub_communities = G.sub_communities\n","KG = G.KG"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T16:37:08.806903Z","iopub.execute_input":"2024-02-29T16:37:08.807631Z","iopub.status.idle":"2024-02-29T16:37:08.811768Z","shell.execute_reply.started":"2024-02-29T16:37:08.807599Z","shell.execute_reply":"2024-02-29T16:37:08.810831Z"},"trusted":true,"id":"_XpXLy089-2u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_subgraph(nodes):\n","    initial_nodes = set(nodes)\n","    first_order_connections = set()\n","    for node in initial_nodes:\n","        first_order_connections.update(KG.neighbors(node))\n","    combined_nodes = initial_nodes.union(first_order_connections)\n","\n","    subgraph = KG.subgraph(combined_nodes)\n","    filtered_edges = [(u, v, d.get(\"title\", \"No title\")) for u, v, d in subgraph.edges(data=True)\n","                      if u in initial_nodes or v in initial_nodes]\n","\n","    return filtered_edges"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:27:28.089653Z","iopub.execute_input":"2024-02-29T09:27:28.089954Z","iopub.status.idle":"2024-02-29T09:27:28.105169Z","shell.execute_reply.started":"2024-02-29T09:27:28.089928Z","shell.execute_reply":"2024-02-29T09:27:28.104461Z"},"trusted":true,"id":"hBFCnAM89-2v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Extracting Insights"],"metadata":{"id":"N2vNQju49-2w"}},{"cell_type":"code","source":["# Using OpenAI\n","\n","SYS_PROMPT_INS = (\n","    \"Embark on a captivating exploration of 'your Second Brain,' a term we use to refer to a comprehensive Knowledge Graph. 'your Second Brain,' is a treasure trove of interconnected information, insights, and data waiting to be unveiled. Our journey is one of discovery, aiming to illuminate the hidden gems and insights that lie within this structured network of knowledge. With a spirit of curiosity, we dive into this rich tapestry of data points, relationships, and patterns, guided by the principles of fidelity and innovation in our exploration. \"\n","    \"Our mission unfolds in distinct phases:\"\n","    \"\\n1. Engage deeply with the essence of 'your Second Brain,' absorbing the intricate patterns, themes, and connections that define its landscape. This involves analyzing the Knowledge Graph to understand its structure, the relationships between entities, and the key concepts it encompasses. \"\n","    \"\\n2. Craft a reflective summary that distills the core ideas and themes from the Knowledge Graph, capturing the essence of 'your Second Brain' in a concise narrative. \"\n","    \"\\n3. Unveil a series of insights or questions that emerge from our engagement with the Knowledge Graph. These insights should spring directly from the fabric of 'your Second Brain,' offering new perspectives and pathways for exploration. \"\n","    \"\\n4. As we navigate this journey, our discoveries will be carefully organized and presented in a JSON format, ensuring clarity and structure in the way we share our findings. \"\n","    \"\\nEmbarking on this adventure, we are tasked with presenting our summary and insights in a structured JSON object with the specified format:\"\n","    \"\\n{\"\n","    \"\\n\\\"Summary\\\": \\\"[Our summary here]\\\",\"\n","    \"\\n\\\"New insights\\\": [\"\n","    \"\\n  \\\"[Insight 1]\\\",\"\n","    \"\\n  \\\"[Insight 2]\\\",\"\n","    \"\\n  \\\"[Insight 3]\\\"\"\n","    \"\\n  // Add more insights as necessary\"\n","    \"\\n]\"\n","    \"\\n}\"\n","    \"\\nBelow is 'your Second Brain.' Let's analyze it with diligence and creativity, encapsulating our reflections and new insights. \"\n","    \"\\nProvide your output in the specified JSON format\"\n",")\n","\n","def insight_prompt(sub_community):\n","    i=0\n","    while True:\n","        try:\n","            graph = client.chat.completions.create(\n","              model=\"gpt-4-0125-preview\",\n","              messages = [\n","                      {\n","                          \"role\": \"system\",\n","                          \"content\": SYS_PROMPT_INS,\n","                      },\n","                      {\n","                          \"role\": \"user\",\n","                          \"content\": f\"your Second Brain: ```{sub_community}``` \\n\\n output: \"\n","                      }\n","                  ]\n","            )\n","            result = graph.choices[0].message.content\n","            start_index = result.find('{\\n')\n","            end_index = result.find('}')\n","            if end_index == -1: end_index = len(result) -1\n","            outputs = result[start_index:end_index+1].replace('\\n', '')\n","            print(outputs)\n","\n","            graph_out = json.loads(outputs)\n","            break\n","\n","        except Exception as e:\n","            print(f\"An error occurred: {e}\")\n","            print(\"Attempting to rerun the code...\")\n","            if i > 10:\n","                print('Fatal too many errors')\n","                break\n","            i+=1\n","            continue\n","    return graph_out"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:48:48.878601Z","iopub.execute_input":"2024-02-29T09:48:48.879475Z","iopub.status.idle":"2024-02-29T09:48:48.889829Z","shell.execute_reply.started":"2024-02-29T09:48:48.879442Z","shell.execute_reply":"2024-02-29T09:48:48.888810Z"},"trusted":true,"id":"Cea55d5H9-2x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Using Zephyr\n","\n","SYS_PROMPT_INS = (\n","    \"Embark on a captivating exploration of 'your Second Brain,' a term we use to refer to a comprehensive Knowledge Graph. 'your Second Brain,' is a treasure trove of interconnected information, insights, and data waiting to be unveiled. Our journey is one of discovery, aiming to illuminate the hidden gems and insights that lie within this structured network of knowledge. With a spirit of curiosity, we dive into this rich tapestry of data points, relationships, and patterns, guided by the principles of fidelity and innovation in our exploration. \"\n","    \"Our mission unfolds in distinct phases:\"\n","    \"\\n1. Engage deeply with the essence of 'your Second Brain,' absorbing the intricate patterns, themes, and connections that define its landscape. This involves analyzing the Knowledge Graph to understand its structure, the relationships between entities, and the key concepts it encompasses. \"\n","    \"\\n2. Craft a reflective summary that distills the core ideas and themes from the Knowledge Graph, capturing the essence of 'your Second Brain' in a concise narrative. \"\n","    \"\\n3. Unveil a series of insights or questions that emerge from our engagement with the Knowledge Graph. These insights should spring directly from the fabric of 'your Second Brain,' offering new perspectives and pathways for exploration. \"\n","    \"\\n4. As we navigate this journey, our discoveries will be carefully organized and presented in a JSON format, ensuring clarity and structure in the way we share our findings. \"\n","    \"\\nEmbarking on this adventure, we are tasked with presenting our summary and insights in a structured JSON object with the specified format:\"\n","    \"\\n{\"\n","    \"\\n\\\"Summary\\\": \\\"[Our summary here]\\\",\"\n","    \"\\n\\\"New insights\\\": [\"\n","    \"\\n  \\\"[Insight 1]\\\",\"\n","    \"\\n  \\\"[Insight 2]\\\",\"\n","    \"\\n  \\\"[Insight 3]\\\"\"\n","    \"\\n  // Add more insights as necessary\"\n","    \"\\n]\"\n","    \"\\n}\"\n","    \"\\nBelow is 'your Second Brain.' Let's analyze it with diligence and creativity, encapsulating our reflections and new insights. \"\n","    \"\\nProvide your output in the specified JSON format\"\n",")\n","\n","\n","def insight_prompt(sub_community):\n","    messages = [\n","          {\n","              \"role\": \"system\",\n","              \"content\": SYS_PROMPT_INS,\n","          },\n","          {\n","              \"role\": \"user\",\n","              \"content\": f\"your Second Brain: ```{sub_community}``` \\n\\n output: \"\n","          },\n","      ]\n","\n","    i=0\n","    while True:\n","        try:\n","            prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","            outputs = pipe(prompt, max_new_tokens=1024, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)[0][\"generated_text\"]\n","\n","            start = '|assistant|>\\n'\n","            start_index = outputs.find(start)\n","            inter = outputs[start_index + len(start):]\n","\n","            start_index = inter.find('{\\n')\n","            end_index = inter.find('}')\n","            if end_index == -1: end_index = len(inter) -1\n","            outputs = inter[start_index:end_index+1].replace('\\n', '')\n","            print(outputs)\n","\n","            output = json.loads(outputs)\n","            break  # Exit the loop if execution is successful\n","\n","        except Exception as e:\n","            print(f\"An error occurred: {e}\")\n","            print(\"Attempting to rerun the code...\")\n","            if i > 10:\n","                print('Fatal too many errors')\n","                break\n","            i+=1\n","    return output"],"metadata":{"execution":{"iopub.status.busy":"2024-02-25T16:17:03.956109Z","iopub.execute_input":"2024-02-25T16:17:03.956799Z","iopub.status.idle":"2024-02-25T16:17:03.968044Z","shell.execute_reply.started":"2024-02-25T16:17:03.956758Z","shell.execute_reply":"2024-02-25T16:17:03.967059Z"},"trusted":true,"id":"RuCwJaNv9-2y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["insight = insight_prompt(get_subgraph(sub_communities[-2][-1]))"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:57:14.720400Z","iopub.execute_input":"2024-02-29T09:57:14.721135Z","iopub.status.idle":"2024-02-29T09:57:32.917634Z","shell.execute_reply.started":"2024-02-29T09:57:14.721106Z","shell.execute_reply":"2024-02-29T09:57:32.916566Z"},"trusted":true,"id":"vDxcO5ed9-2z","outputId":"dcb8a1de-451b-4227-cb5b-64d219f960c2"},"execution_count":null,"outputs":[{"name":"stdout","text":"{  \"Summary\": \"The Knowledge Graph explores the inner workings and structure of transformer models, particularly emphasizing the 'transformer encoder layer' as a nexus point. It delves into how specific components, such as 'data flow', 'residual connections', 'multi-head attention', and 'mlp (Multilayer Perceptron)', interact within the architecture of a transformer model, highlighting the 'encoder' aspect. The graph paints a picture of a highly interconnected system, where 'residual connections' play a crucial role in supporting the flow and processing of data through various mechanisms like 'add & norm'. This structured approach underpins the model's ability to learn complex patterns in data, situating the 'transformer model architecture' within the broader category of 'neural networks'. The inclusion of 'layer normalization' suggests an emphasis on techniques to stabilize and enhance the learning process. Through the lens of 'your Second Brain', the emphasis is on understanding the transformer encoder layer's pivotal role and its associated components in the function and efficacy of transformer models.\",  \"New insights\": [    \"The central role of 'transformer encoder layer' suggests that innovations in transformer model efficiency and effectiveness could heavily rely on enhancing or optimizing this component.\",    \"Given the critical function of 'residual connections' in supporting data flow and their inclusion in the 'transformer encoder layer', there may be unexplored ways to improve these connections for better model performance.\",    \"The interplay between 'multi-head attention', 'mlp', and 'add & norm' within the 'transformer encoder layer' illuminates potential areas for fine-tuning and innovation, possibly by adjusting these components' configurations or interactions.\"  ]}\n","output_type":"stream"}]},{"cell_type":"markdown","source":["# Extracting Recommendations"],"metadata":{"id":"tFNu9iT99-20"}},{"cell_type":"code","source":["# Using OpenAI\n","\n","SYS_PROMPT_REC = (\n","    \"Welcome to a dynamic exploration of 'your Second Brain,' a comprehensive knowledge graph that encapsulates an area of interest with rich detail and interconnected insights. As an ever-curious learner eager to expand my horizons, I'm seeking your expertise to delve into 'your Second Brain' and guide me towards new horizons of knowledge. \"\n","    \"Here's how we'll chart this journey of discovery:\"\n","    \"\\n1. Analyze 'your Second Brain,' paying close attention to its themes, connections, and any gaps in knowledge. \"\n","    \"\\n2. Identify areas for further exploration that would enrich the understanding and breadth of 'your Second Brain.' \"\n","    \"\\n3. Recommend subjects or areas of study that logically extend the knowledge within 'your Second Brain.' Each recommendation should aim to address gaps, expand on current themes, or explore new directions. \"\n","    \"\\n4. Format your output as a list of JSON objects, with each JSON containing a recommendation including a 'title' and a 'description.'\"\n","    \"\\n\\nPlease analyze 'your Second Brain' and provide your recommendations in the specified list of JSONs format.\"\n",")\n","\n","def recommendation_prompt(sub_community):\n","    i=0\n","    while True:\n","        try:\n","            graph = client.chat.completions.create(\n","              model=\"gpt-4-0125-preview\",\n","              messages = [\n","                      {\n","                          \"role\": \"system\",\n","                          \"content\": SYS_PROMPT_REC,\n","                      },\n","                      {\n","                          \"role\": \"user\",\n","                          \"content\": f\"your Second Brain: ```{sub_community}``` \\n\\n output: \"\n","                      }\n","                  ]\n","            )\n","            result = graph.choices[0].message.content\n","            start_index = result.find('[\\n')\n","            end_index = result.find(']')\n","            if end_index == -1: end_index = len(result) -1\n","            outputs = result[start_index:end_index+1].replace('\\n', '')\n","            print(outputs)\n","\n","            graph_out = json.loads(outputs)\n","            break\n","\n","        except Exception as e:\n","            print(f\"An error occurred: {e}\")\n","            print(\"Attempting to rerun the code...\")\n","            if i > 10:\n","                print('Fatal too many errors')\n","                break\n","            i+=1\n","            continue\n","    return graph_out"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:51:39.530927Z","iopub.execute_input":"2024-02-29T09:51:39.531627Z","iopub.status.idle":"2024-02-29T09:51:39.541535Z","shell.execute_reply.started":"2024-02-29T09:51:39.531593Z","shell.execute_reply":"2024-02-29T09:51:39.540592Z"},"trusted":true,"id":"2BEjOB1T9-21"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Using Zephyr\n","\n","SYS_PROMPT_REC = (\n","    \"Welcome to a dynamic exploration of 'your Second Brain,' a comprehensive knowledge graph that encapsulates an area of interest with rich detail and interconnected insights. As an ever-curious learner eager to expand my horizons, I'm seeking your expertise to delve into 'your Second Brain' and guide me towards new horizons of knowledge. \"\n","    \"Here's how we'll chart this journey of discovery:\"\n","    \"\\n1. Analyze 'your Second Brain,' paying close attention to its themes, connections, and any gaps in knowledge. \"\n","    \"\\n2. Identify areas for further exploration that would enrich the understanding and breadth of 'your Second Brain.' \"\n","    \"\\n3. Recommend subjects or areas of study that logically extend the knowledge within 'your Second Brain.' Each recommendation should aim to address gaps, expand on current themes, or explore new directions. \"\n","    \"\\n4. Format your output as a list of JSON objects, with each JSON containing a recommendation including a 'title' and a 'description.'\"\n","    \"\\n\\nPlease analyze 'your Second Brain' and provide your recommendations in the specified list of JSONs format.\"\n",")\n","\n","def recommendation_prompt(sub_community):\n","    messages = [\n","          {\n","              \"role\": \"system\",\n","              \"content\": SYS_PROMPT_REC,\n","          },\n","          {\n","              \"role\": \"user\",\n","              \"content\": f\"your Second Brain: ```{sub_community}``` \\n\\n output: \"\n","          },\n","      ]\n","\n","    i=0\n","    while True:\n","        try:\n","            prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","            outputs = pipe(prompt, max_new_tokens=1024, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)[0][\"generated_text\"]\n","\n","            start = '|assistant|>\\n'\n","            start_index = outputs.find(start)\n","            inter = outputs[start_index + len(start):]\n","\n","            start_index = inter.find('[\\n')\n","            end_index = inter.find(']')\n","            if end_index == -1: end_index = len(inter) -1\n","            outputs = inter[start_index:end_index+1].replace('\\n', '')\n","            print(outputs)\n","\n","            output = json.loads(outputs)\n","            break  # Exit the loop if execution is successful\n","\n","        except Exception as e:\n","            print(f\"An error occurred: {e}\")\n","            print(\"Attempting to rerun the code...\")\n","            if i > 10:\n","                print('Fatal too many errors')\n","                break\n","            i+=1\n","    return output"],"metadata":{"execution":{"iopub.status.busy":"2024-02-25T16:18:02.110917Z","iopub.execute_input":"2024-02-25T16:18:02.111880Z","iopub.status.idle":"2024-02-25T16:18:02.122622Z","shell.execute_reply.started":"2024-02-25T16:18:02.111830Z","shell.execute_reply":"2024-02-25T16:18:02.121345Z"},"trusted":true,"id":"dAqCqIWe9-22"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["recommendation = recommendation_prompt(get_subgraph(sub_communities[-2][-1]))"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:57:37.156284Z","iopub.execute_input":"2024-02-29T09:57:37.157024Z","iopub.status.idle":"2024-02-29T09:57:58.811964Z","shell.execute_reply.started":"2024-02-29T09:57:37.156994Z","shell.execute_reply":"2024-02-29T09:57:58.811007Z"},"trusted":true,"id":"76lw5Mpi9-22","outputId":"ee47640d-1574-4326-d99d-bef756ab1df6"},"execution_count":null,"outputs":[{"name":"stdout","text":"[    {        \"title\": \"Deep Learning Optimization Techniques\",        \"description\": \"Since 'your Second Brain' encompasses various neural network architectures and layers, delving into optimization techniques like Gradient Descent, Adam, L-BFGS, and their roles in training deep neural networks could expand the understanding of how these architectures are efficiently trained and converges to solving complex problems.\"    },    {        \"title\": \"Attention Mechanisms in Deep Learning\",        \"description\": \"Given the emphasis on 'multi-head attention' within the transformer encoder layer, exploring the broader topic of attention mechanisms—including self-attention, global attention, and local attention—could enrich the knowledge of how these models prioritize certain parts of the input data over others, leading to improvements in tasks such as translation, classification, and summarization.\"    },    {        \"title\": \"Transformer Models Beyond Natural Language Processing\",        \"description\": \"While transformers have revolutionized NLP, their application in other domains such as computer vision (Vision Transformers, ViT) and speech recognition suggests an interesting expansion of your Second Brain. Investigating these applications could provide insights into the adaptability and scalability of transformer models.\"    },    {        \"title\": \"Evolution of Neural Network Architectures\",        \"description\": \"A historical perspective on how neural network architectures have evolved—from Perceptrons, to Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and finally to transformer models—could offer a comprehensive understanding of deep learning and the conceptual breakthroughs that have led to current state-of-the-art models.\"    },    {        \"title\": \"Graph Neural Networks (GNNs)\",        \"description\": \"Since 'your Second Brain' involves a discussion on architectures that process sequential or structured data, incorporating knowledge on Graph Neural Networks (GNNs) can expand understanding towards models specifically designed to handle graph-structured data, vastly used in social networks, recommendation systems, and bioinformatics.\"    }]\n","output_type":"stream"}]},{"cell_type":"code","source":["recommendation"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:52:07.374683Z","iopub.execute_input":"2024-02-29T09:52:07.374979Z","iopub.status.idle":"2024-02-29T09:52:07.382268Z","shell.execute_reply.started":"2024-02-29T09:52:07.374952Z","shell.execute_reply":"2024-02-29T09:52:07.381097Z"},"trusted":true,"id":"YpA9gOlW9-23","outputId":"ad7833e2-0d54-4b52-8262-5b62e053b134"},"execution_count":null,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"[{'title': 'Deep Learning in Computer Vision',\n  'description': \"Exploring deep learning applications in computer vision could extend the understanding of vision transformers. Topics like CNNs, object detection, and image segmentation offer foundational knowledge that complements the transformer encoder's approach to handling image class and embeddings.\"},\n {'title': 'Natural Language Processing (NLP)',\n  'description': \"Given the importance of sequence-to-sequence tasks, input sequences, and attention mechanisms, delving into NLP fundamentals and state-of-the-art models can enrich the understanding of transformers' application in language tasks, including tokenization, embedding, and the translation process.\"},\n {'title': 'Transformer Model Variants',\n  'description': 'Exploring various transformer model variants such as GPT (Generative Pre-trained Transformer), BERT (Bidirectional Encoder Representations from Transformers), and T5 (Text-to-Text Transfer Transformer) could provide insights into different architectures and their applications in both vision and language tasks.'},\n {'title': 'Positional Encoding Techniques',\n  'description': 'A focused study on positional encoding techniques would complement the existing knowledge on how transformers maintain the order of input sequences. This could include comparing fixed positional encodings with learnable ones and their impact on model performance.'},\n {'title': 'Sequence Modeling Beyond Transformers',\n  'description': 'Exploring sequence modeling techniques beyond transformers, such as RNNs (Recurrent Neural Networks) and LSTMs (Long Short-Term Memory), could provide a broader perspective on sequence modeling and the evolution towards attention-based architectures.'},\n {'title': 'Embedding Techniques',\n  'description': 'Given the central role of embeddings in representing input tokens, sequence classes, and output vectors, a deeper look into embedding techniques, including Word2Vec, GloVe, and contextual embeddings, would enhance understanding of how these representations contribute to model performance.'},\n {'title': 'Attention Mechanisms',\n  'description': 'Since attention mechanisms, particularly multi-head attention, play a crucial role in transformer models, an in-depth exploration of different types of attention (e.g., self-attention, cross-attention) and their computational efficiency could provide valuable insights.'},\n {'title': 'Advanced Topics in Inferential Statistics',\n  'description': 'Considering the mention of inferential statistics and its relation to positional information, studying statistical models for analyzing sequences and their positions can enhance the understanding of how statistical methods support AI model interpretations and predictions.'}]"},"metadata":{}}]},{"cell_type":"markdown","source":["# Suggesting Link Resources for Recommendations"],"metadata":{"id":"CAdfgZRX9-24"}},{"cell_type":"code","source":["def get_google_links(Query):\n","    result = list(search(Query, num_results=3, advanced=True))\n","    print(len(result))\n","    similar = []\n","    for a in result:\n","        print(sim([Query, a.title]))\n","        if sim([Query, a.title]) > 0.6:\n","            similar.append((a.url, a.title))\n","    return similar"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T10:44:13.864024Z","iopub.execute_input":"2024-02-29T10:44:13.865149Z","iopub.status.idle":"2024-02-29T10:44:13.872517Z","shell.execute_reply.started":"2024-02-29T10:44:13.865105Z","shell.execute_reply":"2024-02-29T10:44:13.871327Z"},"trusted":true,"id":"HGabxT8L9-25"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_google_links(\"Graph Neural Networks (GNNs)\")"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T10:50:09.122286Z","iopub.execute_input":"2024-02-29T10:50:09.123229Z","iopub.status.idle":"2024-02-29T10:50:10.358193Z","shell.execute_reply.started":"2024-02-29T10:50:09.123193Z","shell.execute_reply":"2024-02-29T10:50:10.357207Z"},"trusted":true,"id":"z3qrtNp49-25","outputId":"70fee86a-ad6d-47f1-ffa5-2de3d86c0b57"},"execution_count":null,"outputs":[{"name":"stdout","text":"5\n0.7003785371780396\n0.8791953325271606\n0.8470068573951721\n0.7848224639892578\n0.7553575038909912\n","output_type":"stream"},{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"[('https://distill.pub/2021/gnn-intro',\n  'A Gentle Introduction to Graph Neural Networks - Distill.pub'),\n ('https://neptune.ai/blog/graph-neural-network-and-some-of-gnn-applications',\n  'Graph Neural Network and Some of GNN Applications'),\n ('https://en.wikipedia.org/wiki/Graph_neural_network',\n  'Graph neural network'),\n ('https://blogs.nvidia.com/blog/what-are-graph-neural-networks/',\n  'What Are Graph Neural Networks?'),\n ('https://www.analyticsvidhya.com/blog/2022/03/what-are-graph-neural-networks-and-how-do-they-work/',\n  'What are Graph Neural Networks, and how do they work?')]"},"metadata":{}}]},{"cell_type":"code","source":["from youtube_search import YoutubeSearch\n","def get_youtube_links(Query):\n","    result = YoutubeSearch(Query, max_results=4).to_dict()\n","    print(len(result))\n","    similar = []\n","    for a in result:\n","        print(sim([Query, a['title']]))\n","        print(a['title'])\n","#         if sim([Query, a['title']]) > 0.6:\n","        similar.append((\"https://www.youtube.com/\" + a['url_suffix'], a['title']))\n","    return similar"],"metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-02-29T10:41:59.554760Z","iopub.execute_input":"2024-02-29T10:41:59.555536Z","iopub.status.idle":"2024-02-29T10:41:59.561846Z","shell.execute_reply.started":"2024-02-29T10:41:59.555501Z","shell.execute_reply":"2024-02-29T10:41:59.560775Z"},"trusted":true,"id":"Rkbb8q5m9-3R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_youtube_links(\"Graph Neural Networks (GNNs)\")"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:04:59.043624Z","iopub.execute_input":"2024-02-29T11:04:59.044022Z","iopub.status.idle":"2024-02-29T11:04:59.762280Z","shell.execute_reply.started":"2024-02-29T11:04:59.043994Z","shell.execute_reply":"2024-02-29T11:04:59.761290Z"},"trusted":true,"id":"LA3nIFBf9-3S","outputId":"2c4e9771-3926-4fc3-8501-7a679d13d82e"},"execution_count":null,"outputs":[{"name":"stdout","text":"4\n0.7677381038665771\nGraph Neural Networks - a perspective from the ground up\n0.6808393597602844\nAn Introduction to Graph Neural Networks: Models and Applications\n0.7044997811317444\nGraph Neural Networks: A gentle introduction\n0.6996373534202576\nThe ultimate intro to Graph Neural Networks. Maybe.\n","output_type":"stream"},{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"[('https://www.youtube.com//watch?v=GXhBEj1ZtE8&pp=ygUcR3JhcGggTmV1cmFsIE5ldHdvcmtzIChHTk5zKQ%3D%3D',\n  'Graph Neural Networks - a perspective from the ground up'),\n ('https://www.youtube.com//watch?v=zCEYiCxrL_0&pp=ygUcR3JhcGggTmV1cmFsIE5ldHdvcmtzIChHTk5zKQ%3D%3D',\n  'An Introduction to Graph Neural Networks: Models and Applications'),\n ('https://www.youtube.com//watch?v=xFMhLp52qKI&pp=ygUcR3JhcGggTmV1cmFsIE5ldHdvcmtzIChHTk5zKQ%3D%3D',\n  'Graph Neural Networks: A gentle introduction'),\n ('https://www.youtube.com//watch?v=me3UsMm9QEs&pp=ygUcR3JhcGggTmV1cmFsIE5ldHdvcmtzIChHTk5zKQ%3D%3D',\n  'The ultimate intro to Graph Neural Networks. Maybe.')]"},"metadata":{}}]},{"cell_type":"markdown","source":["# Visualizing the Built KG with its Communities"],"metadata":{"id":"_PpxuvOA9-3T"}},{"cell_type":"code","source":["import seaborn as sns\n","import random\n","palette = \"hls\"\n","\n","## Now add these colors to communities and make another dataframe\n","def colors2Community(communities) -> pd.DataFrame:\n","    ## Define a color palette\n","    p = sns.color_palette(palette, len(communities)).as_hex()\n","    random.shuffle(p)\n","    rows = []\n","    group = 0\n","    for community in communities:\n","        color = p.pop()\n","        group += 1\n","        for node in community:\n","            rows += [{\"node\": node, \"color\": color, \"group\": group}]\n","    df_colors = pd.DataFrame(rows)\n","    return df_colors\n","\n","\n","colors = colors2Community(communities)\n","colors"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:39:32.906675Z","iopub.execute_input":"2024-02-29T09:39:32.907089Z","iopub.status.idle":"2024-02-29T09:39:33.085603Z","shell.execute_reply.started":"2024-02-29T09:39:32.907058Z","shell.execute_reply":"2024-02-29T09:39:33.084652Z"},"trusted":true,"id":"LmBihZws9-3U","outputId":"f34cfc72-4029-4335-c927-e22cf009cef9"},"execution_count":null,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                        node    color  group\n0                       1897  #91db57      1\n1                       1900  #91db57      1\n2                       1901  #91db57      1\n3                       1905  #91db57      1\n4                       1915  #91db57      1\n..                       ...      ...    ...\n404          unusual produce  #a157db      4\n405          white chocolate  #a157db      4\n406                     wine  #a157db      4\n407  wine pairing adventures  #a157db      4\n408                      wok  #a157db      4\n\n[409 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>node</th>\n      <th>color</th>\n      <th>group</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1897</td>\n      <td>#91db57</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1900</td>\n      <td>#91db57</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1901</td>\n      <td>#91db57</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1905</td>\n      <td>#91db57</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1915</td>\n      <td>#91db57</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>404</th>\n      <td>unusual produce</td>\n      <td>#a157db</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>405</th>\n      <td>white chocolate</td>\n      <td>#a157db</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>406</th>\n      <td>wine</td>\n      <td>#a157db</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>407</th>\n      <td>wine pairing adventures</td>\n      <td>#a157db</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>408</th>\n      <td>wok</td>\n      <td>#a157db</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>409 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":["for index, row in colors.iterrows():\n","    KG.nodes[row['node']]['group'] = row['group']\n","    KG.nodes[row['node']]['color'] = row['color']\n","    KG.nodes[row['node']]['size'] = KG.degree[row['node']]"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:39:33.746357Z","iopub.execute_input":"2024-02-29T09:39:33.747590Z","iopub.status.idle":"2024-02-29T09:39:33.790636Z","shell.execute_reply.started":"2024-02-29T09:39:33.747553Z","shell.execute_reply":"2024-02-29T09:39:33.789550Z"},"trusted":true,"id":"fFaFDiNX9-3V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyvis.network import Network\n","\n","graph_output_directory = \"index.html\"\n","\n","net = Network(\n","    notebook=False,\n","    # bgcolor=\"#1a1a1a\",\n","    cdn_resources=\"remote\",\n","    height=\"900px\",\n","    width=\"100%\",\n","    select_menu=False,\n","    # font_color=\"#cccccc\",\n","    filter_menu=False,\n",")\n","\n","net.from_nx(KG)\n","# net.repulsion(node_distance=150, spring_length=400)\n","net.force_atlas_2based(central_gravity=0.015, gravity=-31)\n","# net.barnes_hut(gravity=-18100, central_gravity=5.05, spring_length=380)\n","net.show_buttons(filter_=[\"physics\"])\n","\n","net.show(graph_output_directory, notebook=False)"],"metadata":{"execution":{"iopub.status.busy":"2024-02-29T09:39:34.350229Z","iopub.execute_input":"2024-02-29T09:39:34.351109Z","iopub.status.idle":"2024-02-29T09:39:34.753290Z","shell.execute_reply.started":"2024-02-29T09:39:34.351074Z","shell.execute_reply":"2024-02-29T09:39:34.752366Z"},"trusted":true,"id":"flFg8ifD9-3W","outputId":"f14aed5c-cfee-44b5-e7da-35415dfa890f"},"execution_count":null,"outputs":[{"name":"stdout","text":"index.html\n","output_type":"stream"}]},{"cell_type":"code","source":["import pickle\n","\n","# save graph object to file\n","pickle.dump(KG, open('KG.pickle', 'wb'))"],"metadata":{"execution":{"iopub.status.busy":"2024-02-25T14:44:41.375419Z","iopub.execute_input":"2024-02-25T14:44:41.375807Z","iopub.status.idle":"2024-02-25T14:44:41.382036Z","shell.execute_reply.started":"2024-02-25T14:44:41.375773Z","shell.execute_reply":"2024-02-25T14:44:41.381026Z"},"trusted":true,"id":"U-jLwMoy9-3Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load graph object from file\n","G_ex = pickle.load(open('KG.pickle', 'rb'))"],"metadata":{"execution":{"iopub.status.busy":"2024-02-25T14:45:02.836090Z","iopub.execute_input":"2024-02-25T14:45:02.836977Z","iopub.status.idle":"2024-02-25T14:45:02.842927Z","shell.execute_reply.started":"2024-02-25T14:45:02.836944Z","shell.execute_reply":"2024-02-25T14:45:02.842031Z"},"trusted":true,"id":"1J32kgfB9-3Y"},"execution_count":null,"outputs":[]}]}